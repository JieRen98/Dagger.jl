using Distributed
import Profile
import Base.gc_num
using TaskLocalValues

export timespan_start, timespan_finish

const Timestamp = UInt64

struct ProfilerResult
    samples::Vector{UInt}
    tasks::Vector{UInt}
end
ProfilerResult(samples, tasks::Vector{Task}) =
    ProfilerResult(samples, map(Base.pointer_from_objref, tasks))
ProfilerResult(samples, tasks::Nothing) =
    ProfilerResult(samples, map(Base.pointer_from_objref, UInt[]))

"""
    Timespan

Identifies space (category, id) and time (timeline, start, finish). It also
tracks GC allocations and profiling samples.
"""
struct Timespan
    category::Symbol
    id::Any
    timeline::Any
    start::Timestamp
    finish::Timestamp
    gc_diff::Base.GC_Diff
    profiler_samples::ProfilerResult
end

"An event generated by `timespan_start` or `timespan_finish`."
struct Event{phase}
    category::Symbol
    id::Any
    timeline::Any
    timestamp::Timestamp
    gc_num::Base.GC_Num
    profiler_samples::ProfilerResult
end

@inline Event(phase::Symbol, category::Symbol,
              @nospecialize(id), @nospecialize(tl),
              time, gc_num, prof) =
    Event{phase}(category, id, tl, time, gc_num, prof)

"""
    make_timespan(start::Event, finish::Event) -> Timespan

Creates a `Timespan` given the start and finish `Event`s.
"""
function make_timespan(start::Event, finish::Event)
    @assert start.category == finish.category
    @assert start.id == finish.id

    Timespan(start.category,
             start.id,
             finish.timeline,
             start.timestamp,
             finish.timestamp,
             Base.GC_Diff(finish.gc_num,start.gc_num),
             mix_samples(start.profiler_samples, finish.profiler_samples))
end

get_logs!(ctx; kwargs...) = get_logs!(log_sink(ctx); kwargs...)

"""
    NoOpLog

Disables event logging entirely.
"""
struct NoOpLog end

function write_event(::NoOpLog, event::Event)
end

get_logs!(::NoOpLog) = nothing

struct FilterLog
    f::Function
    inner_chan::Any
end

function write_event(c::FilterLog, event)
    if c.f(event)
        write_event(c.inner_chan, event)
    end
end

get_logs!(f::FilterLog; kwargs...) = get_logs!(f.inner_chan; kwargs...)

function write_event(io::IO, event::Event)
    serialize(io, event)
end

function write_event(chan::Union{RemoteChannel, Channel}, event::Event)
    put!(chan, event)
end

function write_event(arr::AbstractArray, event::Event)
    push!(arr, event)
end

const event_log_lock = Threads.ReentrantLock()

"""
    LocalEventLog

Stores events in a process-local array. Accessing the logs is all-or-nothing;
if multiple consumers call `get_logs!`, they will get different sets of logs.
"""
struct LocalEventLog end

const _local_event_log = Any[]

function write_event(::LocalEventLog, event::Event)
    lock(event_log_lock) do
        write_event(_local_event_log, event)
    end
end

"""
    get_logs!(::LocalEventLog, raw=false; only_local=false) -> Union{Vector{Timespan},Vector{Event}}

Get the logs from each process' local event log, clearing it in the process.
Set `raw` to `true` to get potentially unmatched `Event`s; the default is to
return only matched events as `Timespan`s. If `only_local` is set `true`, only
process-local logs will be fetched; the default is to fetch logs from all
processes.
"""
function get_logs!(::LocalEventLog; raw=false, only_local=false)
    logs = Dict()
    wkrs = only_local ? myid() : procs()
    # FIXME: Log this logic
    @sync for p in wkrs
        @async logs[p] = remotecall_fetch(p) do
            lock(event_log_lock) do
                log = copy(_local_event_log)
                empty!(_local_event_log)
                log
            end
        end
    end
    if raw
        return logs
    else
        spans = build_timespans(vcat(values(logs)...)).completed
        return convert(Vector{Timespan}, spans)
    end
end
get_logs!(l::LocalEventLog, raw::Bool; kwargs...) = get_logs!(l; raw=raw, kwargs...)

mutable struct MultiEventLogState
    consumers::Dict{Symbol,Any}
    consumer_logs::Dict{Symbol,Vector}
    aggregators::Dict{Symbol,Any}
end
MultiEventLogState() = MultiEventLogState(Dict{Symbol,Any}(),
                                          Dict{Symbol,Vector}(),
                                          Dict{Symbol,Any}())

const MultiEventLogState_PLS = Dict{UInt64,MultiEventLogState}()

"""
    MultiEventLog

Processes events immediately, generating multiple log streams. Multiple
consumers may register themselves in the `MultiEventLog`, and when accessed,
log events will be provided to all consumers. A consumer is simply a function
or callable struct which will be called with an event when it's generated. The
return value of the consumer will be pushed into a log stream dedicated to that
consumer. Errors thrown by consumers will be caught and rendered, but will not
otherwise interrupt consumption by other consumers, or future consumption
cycles. An error will result in `nothing` being appended to that consumer's
log.
"""
struct MultiEventLog
    uid::UInt64
    consumers::Dict{Symbol,Any}
    aggregators::Dict{Symbol,Any}
end
MultiEventLog() = MultiEventLog(rand(UInt64), Dict{Symbol,Any}(), Dict{Symbol,Any}())

function Base.setindex!(ml::MultiEventLog, c, name::Symbol)
    ml.consumers[name] = c
end

function get_state(ml::MultiEventLog)
    lock(event_log_lock) do
        mls = get!(()->MultiEventLogState(), MultiEventLogState_PLS, ml.uid)
        max_length = reduce(max, map(length, values(mls.consumer_logs)); init=0)
        for name in keys(ml.consumers)
            if !haskey(mls.consumers, name)
                mls.consumers[name] = init_similar(ml.consumers[name])
                mls.consumer_logs[name] = Vector{Any}(fill(nothing, max_length))
            end
        end
        for name in keys(ml.aggregators)
            if !haskey(mls.aggregators, name)
                mls.aggregators[name] = init_similar(ml.aggregators[name])
            end
        end
        # FIXME: Remove deleted consumers and aggregators
        mls
    end
end

"Creates a copy of `x` with the same configuration, but fresh/empty data."
init_similar(x) = x

function write_event(ml::MultiEventLog, event::Event)
    mls = get_state(ml)
    lock(event_log_lock) do
        for name in keys(mls.consumers)
            cevent = try
                mls.consumers[name](event)
            catch err
                @error "Error during event consumption:" exception=(err,catch_backtrace())
                nothing
            end
            push!(mls.consumer_logs[name], cevent)
        end
        for name in keys(mls.aggregators)
            try
                mls.aggregators[name](mls.consumer_logs)
            catch err
                @error "Error during log aggregation:" exception=(err,catch_backtrace())
                nothing
            end
        end
    end
end

function get_logs!(ml::MultiEventLog; only_local=false)
    logs = Dict{Int,Dict{Symbol,Vector}}()
    wkrs = only_local ? myid() : procs()
    # FIXME: Log this logic
    @sync for p in wkrs
        @async begin
            logs[p] = remotecall_fetch(p, ml) do ml
                mls = get_state(ml)
                lock(event_log_lock) do
                    sublogs = Dict{Symbol,Vector}()
                    for name in keys(mls.consumers)
                        sublogs[name] = mls.consumer_logs[name]
                        mls.consumer_logs[name] = []
                    end
                    sublogs
                end
            end
        end
    end
    return logs
end

# Core logging operations

const empty_prof = ProfilerResult(UInt[], UInt[])

const prof_refcount = Ref{Threads.Atomic{Int}}(Threads.Atomic{Int}(0))
const prof_lock = Threads.ReentrantLock()
const prof_tasks = TaskLocalValue{Vector{Task}}(()->Task[])

prof_task_put!(task::Task=Base.current_task()) = push!(prof_tasks[], task)
function prof_tasks_peek()
    # FIXME: This is an implementation detail of TaskLocalValue
    if haskey(task_local_storage(), prof_tasks)
        return prof_tasks[]
    else
        return nothing
    end
end

log_sink(ctx) = NoOpLog()
profile(ctx, category, id, tl) = false

"""
    timespan_start(ctx, category::Symbol, id, tl)

Generates an `Event{:start}` which denotes the start of an event. The event is
categorized by `category`, and uniquely identified by `id`; these two must be
the same passed to `timespan_finish` to close the event. `tl` is the "timeline"
of the event, which is just an arbitrary payload attached to the event.
"""
function timespan_start(ctx, category::Symbol, @nospecialize(id), @nospecialize(tl))
    sink = log_sink(ctx)
    isa(sink, NoOpLog) && return
    do_profile = profile(ctx, category, id, tl)
    if do_profile && Threads.atomic_add!(prof_refcount[], 1) == 0
        @lock prof_lock Profile.start_timer()
    end
    ev = Event(:start, category, id, tl, time_ns(), gc_num(), empty_prof)
    write_event(sink, ev)
    nothing
end

"""
    timespan_finish(ctx, category::Symbol, id, tl)

Generates an `Event{:finish}` which denotes the end of an event. The event is
categorized by `category`, and uniquely identified by `id`; these two must be
the same as previously passed to `timespan_start`. `tl` is the "timeline" of
the event, which is just an arbitrary payload attached to the event.
"""
function timespan_finish(ctx, category::Symbol, @nospecialize(id), @nospecialize(tl); tasks=nothing)
    sink = log_sink(ctx)
    isa(sink, NoOpLog) && return
    do_profile = profile(ctx, category, id, tl)
    time = time_ns()
    gcn = gc_num()
    prof = UInt[]
    lidict = Dict{UInt64, Vector{Base.StackTraces.StackFrame}}()
    if tasks === nothing
        tasks = prof_tasks_peek()
        if tasks === nothing
            tasks = [current_task()]
        end
    end
    GC.@preserve tasks begin
        if do_profile
            @lock prof_lock begin
                prof_done = Threads.atomic_sub!(prof_refcount[], 1) == 1
                if prof_done
                    Profile.stop_timer()
                end
                prof = Profile.fetch(;include_meta=true)
                prof = tasks !== nothing ? filter_profile_data(prof, tasks) : prof
            end
        end
        ev = Event(:finish, category, id, tl, time, gcn, ProfilerResult(prof, tasks))
        write_event(sink, ev)
    end
    nothing
end

function filter_profile_data(prof, tasks::Vector{UInt})
    newprof = UInt[]
    start_idx = 1
    for idx in 1:length(prof)
        if idx >= Profile.nmeta && Profile.is_block_end(prof, idx)
            task = prof[idx - Profile.META_OFFSET_TASKID]
            if task in tasks
                append!(newprof, prof[start_idx:idx])
            end
            start_idx = idx+1
        end
    end
    return newprof
end
filter_profile_data(prof, tasks::Vector{Task}) =
    filter_profile_data(prof, map(x->UInt(Base.pointer_from_objref(x)), tasks))

"""
Overall state used during visualization
"""
mutable struct State
    start_events::Dict # (category, id) => Event
    finish_events::Dict  # (category, id) => Event
    #completed::Dict      # timeline => category => Array
    completed::Vector
    start_time::Timestamp
    finish_time::Timestamp
end
State() = State(Dict(), Dict(), Any[], 0, 0)

"""
Add a Timespan to a given State under `tl` (timeline)
and `category`.
"""
function add_span(state, tl, category, span)
    push!(state.completed, span)
    if state.start_time == 0
        state.start_time = span.start
    else
        state.start_time = min(span.start, state.start_time)
    end
    if state.finish_time == 0
        state.finish_time = span.finish
    else
        state.finish_time = max(span.finish, state.finish_time)
    end
    state
end

"""When building state for real-time visualization,
   use next_state to progress gantt state."""
function next_state(state::State, event::Event{:start})
    key = (event.category, event.id)
    if haskey(state.finish_events, key) # finish event reached before start
        span = make_timespan(event, pop!(state.finish_events, key))
        add_span(state, event.timeline, event.category, span)
    else
        state.start_events[key] = event
    end
    state
end

function next_state(state::State, event::Event{:finish})
    key = (event.category, event.id)
    if haskey(state.start_events, key)
        span = make_timespan(pop!(state.start_events, key), event)
        add_span(state, event.timeline, event.category, span)
    else
        state.finish_events[key] = event
    end
    state
end
next_state(state::State, events::AbstractArray) =
    foldl(next_state, events, init=state)

# util

function pushkey(dict, key, thing)
    if haskey(dict, key)
        push!(dict[key],thing)
    else
        dict[key] = Any[thing]
    end
end

function pushkey(dict, key1, args...)
    if haskey(dict, key1)
        pushkey(dict[key1], args...)
    else
        dict[key1] = Dict()
        pushkey(dict[key1], args...)
    end
end

function mix_samples(a,b)
    ProfilerResult(vcat(a.samples, b.samples),
                   unique(vcat(a.tasks, b.tasks)))
end

function build_timespans(events)
    next_state(State(), events)
end

function add_gc_diff(x,y)
    Base.GC_Diff(
        x.allocd     + y.allocd,
        x.malloc     + y.malloc,
        x.realloc    + y.realloc,
        x.poolalloc  + y.poolalloc,
        x.bigalloc   + y.bigalloc,
        x.freecall   + y.freecall,
        x.total_time + y.total_time,
        x.pause      + y.pause,
        x.full_sweep + y.full_sweep
    )
end

function aggregate_events(xs)
    gc_diff = reduce(add_gc_diff, map(x -> x.gc_diff, xs))
    time_spent = sum(map(x -> x.finish - x.start, xs))
    profiler_samples = treereduce(mix_samples, map(x->x.profiler_samples, xs))
    time_spent, gc_diff, profiler_samples
end

function summarize_events(time_spent, gc_diff, profiler_samples)
    Base.time_print(time_spent, gc_diff.allocd, gc_diff.total_time, Base.gc_alloc_count(gc_diff))
    if !isempty(profiler_samples.samples)
        Profile.print(profiler_samples.samples)
    end
end

summarize_events(xs) = summarize_events(aggregate_events(xs)...)
