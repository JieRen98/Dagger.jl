var documenterSearchIndex = {"docs":
[{"location":"dynamic/#Dynamic-Scheduler-Control","page":"Dynamic Scheduler Control","title":"Dynamic Scheduler Control","text":"","category":"section"},{"location":"dynamic/","page":"Dynamic Scheduler Control","title":"Dynamic Scheduler Control","text":"Normally, Dagger executes static graphs defined with delayed and @par. However, it is possible for thunks to dynamically modify the graph at runtime, and to generally exert direct control over the scheduler's internal state. The Dagger.sch_handle function provides this functionality within a thunk:","category":"page"},{"location":"dynamic/","page":"Dynamic Scheduler Control","title":"Dynamic Scheduler Control","text":"function mythunk(x)\n    h = Dagger.sch_handle()\n    Dagger.halt!(h)\n    return x\nend","category":"page"},{"location":"dynamic/","page":"Dynamic Scheduler Control","title":"Dynamic Scheduler Control","text":"The above example prematurely halts a running scheduler at the next opportunity using Dagger.halt!:","category":"page"},{"location":"dynamic/","page":"Dynamic Scheduler Control","title":"Dynamic Scheduler Control","text":"Dagger.halt!","category":"page"},{"location":"dynamic/","page":"Dynamic Scheduler Control","title":"Dynamic Scheduler Control","text":"There are a variety of other built-in functions available for various uses:","category":"page"},{"location":"dynamic/","page":"Dynamic Scheduler Control","title":"Dynamic Scheduler Control","text":"Dagger.get_dag_ids Dagger.add_thunk!","category":"page"},{"location":"dynamic/","page":"Dynamic Scheduler Control","title":"Dynamic Scheduler Control","text":"When working with thunks acquired from get_dag_ids or add_thunk!, you will have ThunkID objects which refer to a thunk by ID. Scheduler control functions which work with thunks accept or return ThunkIDs. For example, one can create a new thunkt and get its result with Base.fetch:","category":"page"},{"location":"dynamic/","page":"Dynamic Scheduler Control","title":"Dynamic Scheduler Control","text":"function mythunk(x)\n    h = Dagger.sch_handle()\n    id = Dagger.add_thunk!(h, x) do y\n        y + 1\n    end\n    return fetch(h, id)\nend","category":"page"},{"location":"dynamic/","page":"Dynamic Scheduler Control","title":"Dynamic Scheduler Control","text":"Alternatively, Base.wait can be used when one does not wish to retrieve the returned value of the thunk.","category":"page"},{"location":"dynamic/","page":"Dynamic Scheduler Control","title":"Dynamic Scheduler Control","text":"Users with needs not covered by the built-in functions should use the Dagger.exec! function to pass a user-defined function, closure, or callable struct to the scheduler, along with a payload which will be provided to that function:","category":"page"},{"location":"dynamic/","page":"Dynamic Scheduler Control","title":"Dynamic Scheduler Control","text":"Dagger.exec!","category":"page"},{"location":"dynamic/","page":"Dynamic Scheduler Control","title":"Dynamic Scheduler Control","text":"Note that all functions called by Dagger.exec! take the scheduler's internal lock, so it's safe to manipulate the internal ComputeState object within the user-provided function.","category":"page"},{"location":"scheduler-internals/#Scheduler-Internals","page":"Scheduler Internals","title":"Scheduler Internals","text":"","category":"section"},{"location":"scheduler-internals/","page":"Scheduler Internals","title":"Scheduler Internals","text":"The scheduler is called Dagger.Sch. It contains a single internal instance of type ComputeState, which maintains all necessary state to represent the set of waiting, ready, and completed (or \"finished\") graph nodes, cached Chunks, and maps of interdependencies between nodes. It uses Julia's task infrastructure to asynchronously send work requests to remote compute processes, and uses a Julia Channel as an inbound queue for completed work. There is an outer loop which drives the scheduler, which continues executing until all nodes in the graph have completed executing and the final result of the graph is ready to be returned to the user. This outer loop continuously performs two main operations: the first is to launch the execution of nodes which have become \"ready\" to execute; the second is to \"finish\" nodes which have been completed.","category":"page"},{"location":"scheduler-internals/#Scheduler-Initialization","page":"Scheduler Internals","title":"Scheduler Initialization","text":"","category":"section"},{"location":"scheduler-internals/","page":"Scheduler Internals","title":"Scheduler Internals","text":"At the very beginning of a scheduler's lifecycle, the ComputeState is elaborated based on the computed sets of dependencies between nodes, and all nodes are placed in a \"waiting\" state. If any of the nodes are found to only have inputs which are not Thunks, then they are moved from \"waiting\" to \"ready\". The set of available \"workers\" (the set of available compute processes located throughout the cluster) is recorded, of size Nworkers.","category":"page"},{"location":"scheduler-internals/#Scheduler-Outer-Loop","page":"Scheduler Internals","title":"Scheduler Outer Loop","text":"","category":"section"},{"location":"scheduler-internals/","page":"Scheduler Internals","title":"Scheduler Internals","text":"At each outer loop iteration, up to Nworkers processes that are currently in the \"ready\" state will be moved into the \"running\" state, and asynchronously sent (along with input arguments) to one of the Nworkers processes for execution. Subsequently, if any nodes exist in the inbound queue (i.e. the nodes have completed execution and their result is stored on the process that executed the node), then the most recently-queued node is removed from the queue, \"finished\", and placed in the \"finished\" state.","category":"page"},{"location":"scheduler-internals/#Node-Execution","page":"Scheduler Internals","title":"Node Execution","text":"","category":"section"},{"location":"scheduler-internals/","page":"Scheduler Internals","title":"Scheduler Internals","text":"Executing a node (here called Ne) in the \"ready\" state comprises two tasks. The first task is to identify which node in the set of \"ready\" nodes will be Ne (the node to execute). This choice is based on a concept known as \"affinity\", which is a cost-based metric used to evaluate the suitability of executing a given node on a given process. The metric is based primarily on the location of the input arguments to the node, as well as the arguments computed size in bytes. A fixed amount of affinity is added for each argument when the process in question houses that argument. Affinity is then added based on some base affinity value multiplied by the argument's size in bytes. The total affinities for each node are then used to pick the most optimal node to execute (typically, the one with the highest affinity).","category":"page"},{"location":"scheduler-internals/","page":"Scheduler Internals","title":"Scheduler Internals","text":"The second task is to prepare and send the node to a process for execution. If the node has been executed in the past (due to it being an argument to multiple other nodes), then the node is finished, and its result is pulled from the cache. If the node has not yet been executed, it is first checked if it is a \"meta\" node. A \"meta\" node is explicitly designated as such by the user or library, and will execute directly on its inputs as chunks (the data contained in the chunks are not immediately retrieved from the processors they reside on). Such a node will be executed directly within the scheduler, under the assumption that such a node is not expensive to execute. If the node is not a \"meta\" node, the executing worker process chooses (in round-robin fashion) a suitable processor to execute to execute the node on, based on the node's function, the input argument types, and user-defined rules for processor selection. The input arguments are then asynchronously transferred (via processor move operation) to the selected processor, and the appropriate call to the processor is made with the function and input arguments. Once execution completes and a result is obtained, it is wrapped as a Chunk, and the Chunk's handle is returned to the scheduler's inbound queue for node finishing.","category":"page"},{"location":"scheduler-internals/#Node-Finishing","page":"Scheduler Internals","title":"Node Finishing","text":"","category":"section"},{"location":"scheduler-internals/","page":"Scheduler Internals","title":"Scheduler Internals","text":"\"Finishing\" a node (here called Nf) performs three main tasks. The first task is to find all of the downstream \"children\" nodes of Nf (the set of nodes which use Nf's result as one of their input arguments) that have had all of their input arguments computed and are in the \"waiting\" state, and move them into the \"ready\" state. The second task is to check all of the inputs to Nf to determine if any of them no longer have children nodes which have not been finished; if such inputs match this pattern, their cached result may be freed by the scheduler to minimize data usage. The third task is to mark Nf as \"finished\", and also to indicate to the scheduler whether another node has become \"ready\" to execute.","category":"page"},{"location":"scheduler-visualization/#Scheduler-Visualization-with-DaggerWebDash","page":"Scheduler Visualization","title":"Scheduler Visualization with DaggerWebDash","text":"","category":"section"},{"location":"scheduler-visualization/","page":"Scheduler Visualization","title":"Scheduler Visualization","text":"When working with Dagger, especially when working with its scheduler, it can be helpful to visualize what Dagger is doing internally. To assist with this, a web dashboard is available in the DaggerWebDash.jl package. This web dashboard uses a web server running within each Dagger worker, along with event logging information, to expose details about the scheduler. Information like worker and processor saturation, memory allocations, profiling traces, and much more are available in easy-to-interpret plots.","category":"page"},{"location":"scheduler-visualization/","page":"Scheduler Visualization","title":"Scheduler Visualization","text":"Using the dashboard is relatively simple and straightforward; if you run Dagger's benchmarking script, it's enabled for you automatically if the BENCHMARK_RENDER environment variable is set to webdash. This is the easiest way to get started with the web dashboard for new users.","category":"page"},{"location":"scheduler-visualization/","page":"Scheduler Visualization","title":"Scheduler Visualization","text":"For manual usage, the following snippet of code will suffice:","category":"page"},{"location":"scheduler-visualization/","page":"Scheduler Visualization","title":"Scheduler Visualization","text":"ctx = Context() # or `ctx = Dagger.Sch.eager_context()` for eager API usage\nml = Dagger.MultiEventLog()\n\n## Add some logging events of interest\n\nml[:core] = Dagger.Events.CoreMetrics()\nml[:id] = Dagger.Events.IDMetrics()\nml[:timeline] = Dagger.Events.TimelineMetrics()\n# ...\n\n# (Optional) Enable profile flamegraph generation with ProfileSVG\nml[:profile] = DaggerWebDash.ProfileMetrics()\nctx.profile = true\n\n# Create a LogWindow; necessary for real-time event updates\nlw = Dagger.Events.LogWindow(20*10^9, :core)\nml.aggregators[:logwindow] = lw\n\n# Create the D3Renderer server on port 8080\nd3r = DaggerWebDash.D3Renderer(8080)\n\n## Add some plots! Rendered top-down in order\n\n# Show an overview of all generated events as a Gantt chart\npush!(d3r, GanttPlot(:core, :id, :timeline, :esat, :psat, \"Overview\"))\n\n# Show various numerical events as line plots over time\npush!(d3r, LinePlot(:core, :wsat, \"Worker Saturation\", \"Running Tasks\"))\npush!(d3r, LinePlot(:core, :loadavg, \"CPU Load Average\", \"Average Running Threads\"))\npush!(d3r, LinePlot(:core, :bytes, \"Allocated Bytes\", \"Bytes\"))\npush!(d3r, LinePlot(:core, :mem, \"Available Memory\", \"% Free\"))\n\n# Show a graph rendering of compute tasks and data movement between them\n# Note: Profile events are ignored if absent from the log\npush!(d3r, GraphPlot(:core, :id, :timeline, :profile, \"DAG\"))\n\n# TODO: Not yet functional\n#push!(d3r, ProfileViewer(:core, :profile, \"Profile Viewer\"))\n\n# Add the D3Renderer as a consumer of special events generated by LogWindow\npush!(lw.creation_handlers, d3r)\npush!(lw.deletion_handlers, d3r)\n\n# D3Renderer is also an aggregator\nml.aggregators[:d3r] = d3r\n\nctx.log_sink = ml\n# ... use `ctx`","category":"page"},{"location":"scheduler-visualization/","page":"Scheduler Visualization","title":"Scheduler Visualization","text":"Once the server has started, you can browse to http://localhost:8080/ (if running on your local machine) to view the plots in real time. The dashboard also provides options at the top of the page to control the drawing speed, enable and disable reading updates from the server (disabling freezes the display at the current instant), and a selector for which worker to look at. If the connection to the server is lost for any reason, the dashboard will attempt to reconnect at 5 second intervals. The dashboard can usually survive restarts of the server perfectly well, although refreshing the page is usually a good idea. Informational messages are also logged to the browser console for debugging.","category":"page"},{"location":"checkpointing/#Checkpointing","page":"Checkpointing","title":"Checkpointing","text":"","category":"section"},{"location":"checkpointing/","page":"Checkpointing","title":"Checkpointing","text":"If at some point during a Dagger computation a thunk throws an error, or if the entire computation dies because the head node hit an OOM or other unexpected error, the entire computation is lost and needs to be started from scratch. This can be unacceptable for scheduling very large/expensive/mission-critical graphs, and for interactive development where errors are common and easily fixable.","category":"page"},{"location":"checkpointing/","page":"Checkpointing","title":"Checkpointing","text":"Robust applications often support \"checkpointing\", where intermediate results are periodically written out to persistent media, or sharded to the rest of the cluster, to allow resuming an interrupted computation from a point later than the original start. Dagger provides infrastructure to perform user-driven checkpointing of intermediate results once they're generated.","category":"page"},{"location":"checkpointing/","page":"Checkpointing","title":"Checkpointing","text":"As a concrete example, imagine that you're developing a numerical algorithm, and distributing it with Dagger. The idea is to sum all the values in a very big matrix, and then get the square root of the absolute value of the sum of sums. Here is what that might look like:","category":"page"},{"location":"checkpointing/","page":"Checkpointing","title":"Checkpointing","text":"X = compute(randn(Blocks(128,128), 1024, 1024))\nY = [delayed(sum)(chunk) for chunk in X.chunks]\ninner(x...) = sqrt(sum(x))\nZ = delayed(inner)(Y...)\nz = collect(Z)","category":"page"},{"location":"checkpointing/","page":"Checkpointing","title":"Checkpointing","text":"Let's pretend that the above calculation of each element in Y takes a full day to run. If you run this, you might realize that if the final sum call returns a negative number, sqrt will throw a DomainError (because sqrt can't accept negative Real inputs). Of course, you forgot to add a call to abs before the call to sqrt! Now, you know how to fix this, but once you do, you'll have to spend another entire day waiting for it to finish! And maybe you fix this one bug and wait a full day for it to finish, and begin adding more very computationally-heavy code (which inevitably has bugs). Those later computations might fail, and if you're running this as a script (maybe under a cluster scheduler like Slurm), you have to restart everything from the very beginning. This is starting to sound pretty wasteful...","category":"page"},{"location":"checkpointing/","page":"Checkpointing","title":"Checkpointing","text":"Thankfully, Dagger has a simple solution to this: checkpointing. With checkpointing, Dagger can be instructed to save intermediate results (maybe the results of computing Y) to a persistent storage medium of your choice. Probably a file on disk, but maybe a database, or even just stored in RAM in a space-efficient form. You also tell Dagger how to restore this data: how to take the result stored in its persistent form, and turn it back into something identical to the original intermediate data that Dagger computed. Then, when the worst happens and a piece of your algorithm throws an error (as above), Dagger will call the restore function and try to materialize those intermediate results that you painstakingly computed, so that you don't need to re-compute them.","category":"page"},{"location":"checkpointing/","page":"Checkpointing","title":"Checkpointing","text":"Let's see how we'd modify the above example to use checkpointing:","category":"page"},{"location":"checkpointing/","page":"Checkpointing","title":"Checkpointing","text":"using Serialization\nX = compute(randn(Blocks(128,128), 1024, 1024))\nY = [delayed(sum; options=Dagger.Sch.ThunkOptions(;\ncheckpoint=(thunk,result)->begin\n    open(\"checkpoint-$idx.bin\", \"w\") do io\n        serialize(io, collect(result))\n    end\nend, restore=(thunk)->begin\n    open(\"checkpoint-$idx.bin\", \"r\") do io\n        Dagger.tochunk(deserialize(io))\n    end\nend))(chunk) for (idx,chunk) in enumerate(X.chunks)]\ninner(x...) = sqrt(sum(x))\nZ = delayed(inner)(Y...)\nz = collect(Z)","category":"page"},{"location":"checkpointing/","page":"Checkpointing","title":"Checkpointing","text":"Two changes were made: first, we enumerate(X.chunks) so that we can get a unique index to identify each chunk; second, we specify a ThunkOptions to delayed with a checkpoint and restore function that is specialized to write or read the given chunk to or from a file on disk, respectively. Notice the usage of collect in the checkpoint function, and the use of Dagger.tochunk in the restore function; Dagger represents intermediate results as Dagger.Chunk objects, so we need to convert between Chunks and the actual data to keep Dagger happy. Performance-sensitive users might consider modifying these methods to store the checkpoint files on the filesystem of the server that currently owns the Chunk, to minimize data transfer times during checkpoint and restore operations.","category":"page"},{"location":"checkpointing/","page":"Checkpointing","title":"Checkpointing","text":"If we run the above code once, we'll still end up waiting a day for Y to be computed, and we'll still get the DomainError from sqrt. However, when we fix the inner function to include that call to abs that was missing, and we re-run this code starting from the creation of Y, we'll find that we don't actually spend a day waiting; we probably spend a few seconds waiting, and end up with our final result! This is because Dagger called the restore function for each element of Y, and was provided a result by the user-specified function, so it skipped re-computing those sums entirely.","category":"page"},{"location":"checkpointing/","page":"Checkpointing","title":"Checkpointing","text":"You might also notice that when you ran this code the first time, you received errors about \"No such file or directory\", or some similar error; this occurs because Dagger always calls the restore function when it exists. In the first run, the checkpoint files don't yet exist, so there's nothing to restore; Dagger reports the thrown error, but keeps moving along, merrily computing the sums of Y. You're welcome to explicitly check if the file exists, and if not, return nothing; then Dagger won't report an annoying error, and will skip the restoration quietly.","category":"page"},{"location":"checkpointing/","page":"Checkpointing","title":"Checkpointing","text":"Of course, you might have a lot of code that looks like this, and may want to also checkpoint the final result of the z = collect(...) call as well. This is just as easy to do:","category":"page"},{"location":"checkpointing/","page":"Checkpointing","title":"Checkpointing","text":"# compute X, Y, Z above ...\nz = collect(Z; options=Dagger.Sch.SchedulerOptions(;\ncheckpoint=(result)->begin\n    open(\"checkpoint-final.bin\", \"w\") do io\n        serialize(io, collect(result))\n    end\nend, restore=()->begin\n    open(\"checkpoint-final.bin\", \"r\") do io\n        Dagger.tochunk(deserialize(io))\n    end\nend))","category":"page"},{"location":"checkpointing/","page":"Checkpointing","title":"Checkpointing","text":"In this case, the entire computation will be skipped if checkpoint-final.bin exists!","category":"page"},{"location":"api/functions/","page":"Functions and Macros","title":"Functions and Macros","text":"CurrentModule = Dagger","category":"page"},{"location":"api/functions/#Functions","page":"Functions and Macros","title":"Functions","text":"","category":"section"},{"location":"api/functions/","page":"Functions and Macros","title":"Functions and Macros","text":"Pages = [\"functions.md\"]","category":"page"},{"location":"api/functions/#General","page":"Functions and Macros","title":"General","text":"","category":"section"},{"location":"api/functions/","page":"Functions and Macros","title":"Functions and Macros","text":"delayed\nspawn\ntochunk\ndomain\ncompute\ndependents\nnoffspring\norder\ntreereduce","category":"page"},{"location":"api/functions/#Dagger.delayed","page":"Functions and Macros","title":"Dagger.delayed","text":"delayed(f; kwargs...)(args...)\n\nCreates a Thunk object which can be executed later, which will call f with args. kwargs controls various properties of the resulting Thunk.\n\n\n\n\n\n","category":"function"},{"location":"api/functions/#Dagger.spawn","page":"Functions and Macros","title":"Dagger.spawn","text":"spawn(f, args...; kwargs...) -> EagerThunk\n\nSpawns a task with f as the function and args as the arguments, returning an EagerThunk. Uses a scheduler running in the background to execute code.\n\nNote that kwargs are passed to the Thunk constructor, and are documented in its docstring.\n\n\n\n\n\n","category":"function"},{"location":"api/functions/#Dagger.tochunk","page":"Functions and Macros","title":"Dagger.tochunk","text":"tochunk(x, proc; persist=false, cache=false) -> Chunk\n\nCreate a chunk from sequential object x which resides on proc.\n\n\n\n\n\n","category":"function"},{"location":"api/functions/#Dagger.domain","page":"Functions and Macros","title":"Dagger.domain","text":"domain(x::T)\n\nReturns metadata about x. This metadata will be in the domain field of a Chunk object when an object of type T is created as the result of evaluating a Thunk.\n\n\n\n\n\n","category":"function"},{"location":"api/functions/#Dagger.compute","page":"Functions and Macros","title":"Dagger.compute","text":"compute(ctx::Context, d::Thunk; options=nothing) -> Chunk\n\nCompute a Thunk - creates the DAG, assigns ranks to nodes for tie breaking and runs the scheduler with the specified options. Returns a Chunk which references the result.\n\n\n\n\n\ncompute(ctx::Context, x::DArray; persist=true, options=nothing)\n\nA DArray object may contain a thunk in it, in which case we first turn it into a Thunk and then compute it.\n\n\n\n\n\n","category":"function"},{"location":"api/functions/#Dagger.dependents","page":"Functions and Macros","title":"Dagger.dependents","text":"dependents(node::Thunk) -> Dict{Union{Thunk,Chunk}, Set{Thunk}}\n\nFind the set of direct dependents for each task.\n\n\n\n\n\n","category":"function"},{"location":"api/functions/#Dagger.noffspring","page":"Functions and Macros","title":"Dagger.noffspring","text":"noffspring(dpents::Dict{Union{Thunk,Chunk}, Set{Thunk}}) -> Dict{Thunk, Int}\n\nRecursively find the number of tasks dependent on each task in the DAG. Takes a Dict as returned by dependents.\n\n\n\n\n\n","category":"function"},{"location":"api/functions/#Dagger.order","page":"Functions and Macros","title":"Dagger.order","text":"order(node::Thunk, ndeps) -> Dict{Thunk,Int}\n\nGiven a root node of the DAG, calculates a total order for tie-breaking.\n\nRoot node gets score 1,\nrest of the nodes are explored in DFS fashion but chunks of each node are explored in order of noffspring, i.e. total number of tasks depending on the result of the said node.\n\nArgs:\n\nnode: root node\nndeps: result of noffspring\n\n\n\n\n\n","category":"function"},{"location":"api/functions/#Dagger.treereduce","page":"Functions and Macros","title":"Dagger.treereduce","text":"Tree reduce\n\n\n\n\n\n","category":"function"},{"location":"api/functions/#Processors","page":"Functions and Macros","title":"Processors","text":"","category":"section"},{"location":"api/functions/","page":"Functions and Macros","title":"Functions and Macros","text":"execute!\niscompatible\ndefault_enabled\nget_processors\nget_parent\nmove\ncapacity\nget_tls\nset_tls!","category":"page"},{"location":"api/functions/#Dagger.execute!","page":"Functions and Macros","title":"Dagger.execute!","text":"execute!(proc::Processor, f, args...) -> Any\n\nExecutes the function f with arguments args on processor proc. This function can be overloaded by Processor subtypes to allow executing function calls differently than normal Julia.\n\n\n\n\n\n","category":"function"},{"location":"api/functions/#Dagger.iscompatible","page":"Functions and Macros","title":"Dagger.iscompatible","text":"iscompatible(proc::Processor, opts, f, Targs...) -> Bool\n\nIndicates whether proc can execute f over Targs given opts. Processor subtypes should overload this function to return true if and only if it is essentially guaranteed that f(::Targs...) is supported. Additionally, iscompatible_func and iscompatible_arg can be overriden to determine compatibility of f and Targs individually. The default implementation returns false.\n\n\n\n\n\n","category":"function"},{"location":"api/functions/#Dagger.default_enabled","page":"Functions and Macros","title":"Dagger.default_enabled","text":"default_enabled(proc::Processor) -> Bool\n\nReturns whether processor proc is enabled by default (opt-out). Processor subtypes can override this function to make themselves opt-in (default returns false).\n\n\n\n\n\n","category":"function"},{"location":"api/functions/#Dagger.get_processors","page":"Functions and Macros","title":"Dagger.get_processors","text":"get_processors(proc::Processor) -> Vector{T} where T<:Processor\n\nReturns the full list of processors contained in proc, if any. Processor subtypes should overload this function if they can contain sub-processors. The default method will return a Vector containing proc itself.\n\n\n\n\n\n","category":"function"},{"location":"api/functions/#Dagger.get_parent","page":"Functions and Macros","title":"Dagger.get_parent","text":"get_parent(proc::Processor) -> Processor\n\nReturns the parent processor for proc. The ultimate parent processor is an OSProc. Processor subtypes should overload this to return their most direct parent.\n\n\n\n\n\n","category":"function"},{"location":"api/functions/#Dagger.move","page":"Functions and Macros","title":"Dagger.move","text":"move(from_proc::Processor, to_proc::Processor, x)\n\nMoves and/or converts x such that it's available and suitable for usage on the to_proc processor. This function can be overloaded by Processor subtypes to transport arguments and convert them to an appropriate form before being used for exection. Subtypes of Processor wishing to implement efficient data movement should provide implementations where x::Chunk.\n\n\n\n\n\n","category":"function"},{"location":"api/functions/#Dagger.capacity","page":"Functions and Macros","title":"Dagger.capacity","text":"capacity(proc::Processor=OSProc()) -> Int\n\nReturns the total processing capacity of proc.\n\n\n\n\n\n","category":"function"},{"location":"api/functions/#Dagger.get_tls","page":"Functions and Macros","title":"Dagger.get_tls","text":"get_tls()\n\nGets all Dagger TLS variable as a NamedTuple.\n\n\n\n\n\n","category":"function"},{"location":"api/functions/#Dagger.set_tls!","page":"Functions and Macros","title":"Dagger.set_tls!","text":"set_tls!(tls)\n\nSets all Dagger TLS variables from the NamedTuple tls.\n\n\n\n\n\n","category":"function"},{"location":"api/functions/#Context","page":"Functions and Macros","title":"Context","text":"","category":"section"},{"location":"api/functions/","page":"Functions and Macros","title":"Functions and Macros","text":"addprocs!\nrmprocs!","category":"page"},{"location":"api/functions/#Dagger.addprocs!","page":"Functions and Macros","title":"Dagger.addprocs!","text":"addprocs!(ctx::Context, xs)\n\nAdd new workers xs to ctx.\n\nWorkers will typically be assigned new tasks in the next scheduling iteration if scheduling is ongoing.\n\nWorkers can be either Processors or the underlying process IDs as Integers.\n\n\n\n\n\n","category":"function"},{"location":"api/functions/#Dagger.rmprocs!","page":"Functions and Macros","title":"Dagger.rmprocs!","text":"rmprocs!(ctx::Context, xs)\n\nRemove the specified workers xs from ctx.\n\nWorkers will typically finish all their assigned tasks if scheduling is ongoing but will not be assigned new tasks after removal.\n\nWorkers can be either Processors or the underlying process IDs as Integers.\n\n\n\n\n\n","category":"function"},{"location":"api/functions/#Logging","page":"Functions and Macros","title":"Logging","text":"","category":"section"},{"location":"api/functions/","page":"Functions and Macros","title":"Functions and Macros","text":"get_logs!","category":"page"},{"location":"api/functions/#Dagger.get_logs!","page":"Functions and Macros","title":"Dagger.get_logs!","text":"get_logs!(::LocalEventLog, raw=false; only_local=false) -> Union{Vector{Timespan},Vector{Event}}\n\nGet the logs from each process' local event log, clearing it in the process. Set raw to true to get potentially unmatched Events; the default is to return only matched events as Timespans. If only_local is set true, only process-local logs will be fetched; the default is to fetch logs from all processes.\n\n\n\n\n\n","category":"function"},{"location":"api/functions/#Thunk-Execution-Environment","page":"Functions and Macros","title":"Thunk Execution Environment","text":"","category":"section"},{"location":"api/functions/","page":"Functions and Macros","title":"Functions and Macros","text":"These functions are used within the function called by a Thunk.","category":"page"},{"location":"api/functions/","page":"Functions and Macros","title":"Functions and Macros","text":"in_thunk\nthunk_processor","category":"page"},{"location":"api/functions/#Dagger.in_thunk","page":"Functions and Macros","title":"Dagger.in_thunk","text":"in_thunk()\n\nReturns true if currently in a Thunk process, else false.\n\n\n\n\n\n","category":"function"},{"location":"api/functions/#Dagger.thunk_processor","page":"Functions and Macros","title":"Dagger.thunk_processor","text":"thunk_processor()\n\nGet the current processor executing the current thunk.\n\n\n\n\n\n","category":"function"},{"location":"api/functions/#Dynamic-Scheduler-Control","page":"Functions and Macros","title":"Dynamic Scheduler Control","text":"","category":"section"},{"location":"api/functions/","page":"Functions and Macros","title":"Functions and Macros","text":"These functions query and control the scheduler remotely.","category":"page"},{"location":"api/functions/","page":"Functions and Macros","title":"Functions and Macros","text":"Sch.sch_handle\nSch.add_thunk!\nBase.fetch\nBase.wait\nSch.exec!\nSch.halt!\nSch.get_dag_ids","category":"page"},{"location":"api/functions/#Dagger.Sch.sch_handle","page":"Functions and Macros","title":"Dagger.Sch.sch_handle","text":"Gets the scheduler handle for the currently-executing thunk.\n\n\n\n\n\n","category":"function"},{"location":"api/functions/#Dagger.Sch.add_thunk!","page":"Functions and Macros","title":"Dagger.Sch.add_thunk!","text":"Adds a new Thunk to the DAG.\n\n\n\n\n\n","category":"function"},{"location":"api/functions/#Base.fetch","page":"Functions and Macros","title":"Base.fetch","text":"fetch(d::DTable)\n\nCollects all the chunks in the DTable into a single, non-distributed instance of the underlying table type.\n\nFetching an empty DTable results in returning an empty NamedTuple regardless of the underlying tabletype.\n\n\n\n\n\nfetch(d::DTable, sink)\n\nCollects all the chunks in the DTable into a single, non-distributed instance of table type created using the provided sink function.\n\n\n\n\n\nWaits on a thunk to complete, and fetches its result.\n\n\n\n\n\n","category":"function"},{"location":"api/functions/#Base.wait","page":"Functions and Macros","title":"Base.wait","text":"Waits on a thunk to complete.\n\n\n\n\n\n","category":"function"},{"location":"api/functions/#Dagger.Sch.exec!","page":"Functions and Macros","title":"Dagger.Sch.exec!","text":"Executes an arbitrary function within the scheduler, returning the result.\n\n\n\n\n\n","category":"function"},{"location":"api/functions/#Dagger.Sch.halt!","page":"Functions and Macros","title":"Dagger.Sch.halt!","text":"Commands the scheduler to halt execution immediately.\n\n\n\n\n\n","category":"function"},{"location":"api/functions/#Dagger.Sch.get_dag_ids","page":"Functions and Macros","title":"Dagger.Sch.get_dag_ids","text":"Returns all Thunks IDs as a Dict, mapping a Thunk to its downstream dependents.\n\n\n\n\n\n","category":"function"},{"location":"api/functions/#Arrays","page":"Functions and Macros","title":"Arrays","text":"","category":"section"},{"location":"api/functions/","page":"Functions and Macros","title":"Functions and Macros","text":"alignfirst\nview","category":"page"},{"location":"api/functions/#Dagger.alignfirst","page":"Functions and Macros","title":"Dagger.alignfirst","text":"alignfirst(a) -> ArrayDomain\n\nMake a subdomain a standalone domain.\n\nExample\n\njulia> alignfirst(ArrayDomain(11:25, 21:100))\nArrayDomain((1:15), (1:80))\n\n\n\n\n\n","category":"function"},{"location":"api/functions/#Base.view","page":"Functions and Macros","title":"Base.view","text":"view(c::DArray, d)\n\nA view of a DArray chunk returns a DArray of Thunks.\n\n\n\n\n\n","category":"function"},{"location":"api/functions/#File-IO","page":"Functions and Macros","title":"File IO","text":"","category":"section"},{"location":"api/functions/","page":"Functions and Macros","title":"Functions and Macros","text":"warning: Warning\nThese APIs are currently untested and may be removed or modified.","category":"page"},{"location":"api/functions/","page":"Functions and Macros","title":"Functions and Macros","text":"save\nload","category":"page"},{"location":"api/functions/#Dagger.save","page":"Functions and Macros","title":"Dagger.save","text":"save(io::IO, val)\n\nSave a value into the IO buffer. In the case of arrays and sparse matrices, this will save it in a memory-mappable way.\n\nload(io::IO, t::Type, domain) will load the object given its domain\n\n\n\n\n\nsave(ctx, chunk::Union{Chunk, Thunk}, file_path::AbsractString)\n\nSave a chunk to a file at file_path.\n\n\n\n\n\nsave(ctx, chunk, file_path)\n\nSpecial case distmem writing - write to disk on the process with the chunk.\n\n\n\n\n\n","category":"function"},{"location":"api/functions/#Dagger.load","page":"Functions and Macros","title":"Dagger.load","text":"load(ctx::Context, file_path)\n\nLoad an Union{Chunk, Thunk} from a file.\n\n\n\n\n\nload(ctx::Context, ::Type{Chunk}, fpath, io)\n\nLoad a Chunk object from a file, the file path is required for creating a FileReader object\n\n\n\n\n\n","category":"function"},{"location":"api/functions/#Macros","page":"Functions and Macros","title":"Macros","text":"","category":"section"},{"location":"api/functions/","page":"Functions and Macros","title":"Functions and Macros","text":"@par\n@spawn","category":"page"},{"location":"api/functions/#Dagger.@par","page":"Functions and Macros","title":"Dagger.@par","text":"@par [opts] f(args...) -> Thunk\n\nConvenience macro to call Dagger.delayed on f with arguments args. May also be called with a series of assignments like so:\n\nx = @par begin\n    a = f(1,2)\n    b = g(a,3)\n    h(a,b)\nend\n\nx will hold the Thunk representing h(a,b); additionally, a and b will be defined in the same local scope and will be equally accessible for later calls.\n\nOptions to the Thunk can be set as opts with namedtuple syntax, e.g. single=1. Multiple options may be provided, and will be applied to all generated thunks.\n\n\n\n\n\n","category":"macro"},{"location":"api/functions/#Dagger.@spawn","page":"Functions and Macros","title":"Dagger.@spawn","text":"@spawn [opts] f(args...) -> Thunk\n\nConvenience macro like Dagger.@par, but eagerly executed from the moment it's called (equivalent to spawn).\n\nSee the docs for @par for more information and usage examples.\n\n\n\n\n\n","category":"macro"},{"location":"benchmarking/#Benchmarking-Dagger","page":"Benchmarking","title":"Benchmarking Dagger","text":"","category":"section"},{"location":"benchmarking/","page":"Benchmarking","title":"Benchmarking","text":"For ease of benchmarking changes to Dagger's scheduler and the DArray, a benchmarking script exists at benchmarks/benchmark.jl. This script currently allows benchmarking a non-negative matrix factorization (NNMF) algorithm, which we've found to be a good evaluator of scheduling performance. The benchmark script can test with and without Dagger, and also has support for using CUDA or AMD GPUs to accelerate the NNMF via DaggerGPU.jl.","category":"page"},{"location":"benchmarking/","page":"Benchmarking","title":"Benchmarking","text":"The script checks for a number of environment variables, which are used to control the benchmarks that are performed (all of which are optional):","category":"page"},{"location":"benchmarking/","page":"Benchmarking","title":"Benchmarking","text":"BENCHMARK_PROCS: Selects the number of Julia processes and threads to start-up. Specified as 8:4, this option would start 8 extra Julia processes, with 4 threads each. Defaults to 2 processors and 1 thread each.\nBENCHMARK_REMOTES: Specifies a colon-separated list of remote servers to connect to and start Julia processes on, using BENCHMARK_PROCS to indicate the processor/thread configuration of those remotes. Disabled by default (uses the local machine).\nBENCHMARK_OUTPUT_FORMAT: Selects the output format for benchmark results. Defaults to jls, which uses Julia's Serialization stdlib, and can also be jld to use JLD.jl.\nBENCHMARK_RENDER: Configures rendering, which is disabled by default. Can be \"live\" or \"offline\", which are explained below.\nBENCHMARK: Specifies the set of benchmarks to run as a comma-separated list, where each entry can be one of cpu, cuda, or amdgpu, and may optionally append +dagger (like cuda+dagger) to indicate whether or not to use Dagger. Defaults to cpu,cpu+dagger, which runs CPU benchmarks with and without Dagger.\nBENCHMARK_SCALE: Determines how much to scale the benchmark sizing by, typically specified as a UnitRange{Int}. Defaults to 1:5:50, which runs each scale from 1 to 50, in steps of 5.","category":"page"},{"location":"benchmarking/#Rendering-with-BENCHMARK_RENDER","page":"Benchmarking","title":"Rendering with BENCHMARK_RENDER","text":"","category":"section"},{"location":"benchmarking/","page":"Benchmarking","title":"Benchmarking","text":"Dagger contains visualization code for the scheduler (as a Gantt chart) and thunk execution profiling (flamechart), which can be enabled with BENCHMARK_RENDER. Additionally, rendering can be done \"live\", served via a Mux.jl webserver run locally, or \"offline\", where the visualization will be embedded into the results output file. By default, rendering is disabled. If BENCHMARK_RENDER is set to live, a Mux webserver is started at localhost:8000 (the address is not yet configurable), and the Gantt chart and profiling flamechart will be rendered once the benchmarks start. If set to offline, data visualization will happen in the background, and will be passed in the results file.","category":"page"},{"location":"benchmarking/","page":"Benchmarking","title":"Benchmarking","text":"Note that Gantt chart and flamechart output is only generated and relevant during Dagger execution.","category":"page"},{"location":"benchmarking/#TODO:-Plotting","page":"Benchmarking","title":"TODO: Plotting","text":"","category":"section"},{"location":"logging/#Logging-and-Graphing","page":"Logging and Graphing","title":"Logging and Graphing","text":"","category":"section"},{"location":"logging/","page":"Logging and Graphing","title":"Logging and Graphing","text":"Dagger's scheduler keeps track of the important and potentially expensive actions it does, such as moving data between workers or executing thunks, and tracks how much time and memory allocations these operations consume, among other things. Saving this information somewhere accessible is disabled by default, but it's quite easy to turn it on, by setting a \"log sink\" in the Context being used, as ctx.log_sink. A variety of log sinks are built-in to Dagger; the NoOpLog is the default log sink when one isn't explicitly specified, and disables logging entirely (to minimize overhead). There are currently two other log sinks of interest; the first and newer of the two is the Dagger.MultiEventLog, which generates multiple independent log streams, one per \"consumer\" (details in the next section). The second and older sink is the Dagger.LocalEventLog, which is explained later in this document. Most users are recommended to use the MultiEventLog since it's far more flexible and extensible, and is more performant in general.","category":"page"},{"location":"logging/#MultiEventLog","page":"Logging and Graphing","title":"MultiEventLog","text":"","category":"section"},{"location":"logging/","page":"Logging and Graphing","title":"Logging and Graphing","text":"The MultiEventLog is intended to be configurable to exclude unnecessary information, and to include any built-in or user-defined metrics. It stores a set of \"sub-log\" streams internally, appending a single element to each of them when an event is generated. This element can be called a \"sub-event\" (to distinguish it from the higher-level \"event\" that Dagger creates), and is created by a \"consumer\". A consumer is a function or callable struct that, when called with the Dagger.Event object generated by Dagger, returns a sub-event characterizing whatever information the consumer represents. For example, the Dagger.Events.BytesAllocd consumer calculates the total bytes allocated and live at any given time within Dagger, and returns the current value when called. Let's construct one:","category":"page"},{"location":"logging/","page":"Logging and Graphing","title":"Logging and Graphing","text":"ctx = Context()\nml = Dagger.MultiEventLog()\n\n# Add the BytesAllocd consumer to the log as `:bytes`\nml[:bytes] = Dagger.Events.BytesAllocd()\n\nctx.log_sink = ml","category":"page"},{"location":"logging/","page":"Logging and Graphing","title":"Logging and Graphing","text":"As we can see above, each consumer gets a unique name as a Symbol that identifies it. Now that the log sink is attached with a consumer, we can execute some Dagger tasks, and then collect the sub-events generated by BytesAllocd:","category":"page"},{"location":"logging/","page":"Logging and Graphing","title":"Logging and Graphing","text":"# Using the lazy API, for explanatory purposes\ncollect(ctx, delayed(+)(1, delayed(*)(3, 4))) # Allocates 8 bytes\nlog = Dagger.get_logs!(ml)[1] # Get the logs for worker 1\n@show log[:bytes]","category":"page"},{"location":"logging/","page":"Logging and Graphing","title":"Logging and Graphing","text":"You'll then see that 8 bytes are allocated and then freed during the process of executing and completing those tasks.","category":"page"},{"location":"logging/","page":"Logging and Graphing","title":"Logging and Graphing","text":"Note that the MultiEventLog can also be used perfectly well when using Dagger's eager API:","category":"page"},{"location":"logging/","page":"Logging and Graphing","title":"Logging and Graphing","text":"ctx = Dagger.Sch.eager_context()\nctx.log_sink = ml\n\na = Dagger.@spawn 3*4\nDagger.@spawn 1+a","category":"page"},{"location":"logging/","page":"Logging and Graphing","title":"Logging and Graphing","text":"There are a variety of other consumers built-in to Dagger, under the Dagger.Events module:","category":"page"},{"location":"logging/","page":"Logging and Graphing","title":"Logging and Graphing","text":"Dagger.Events.CoreMetrics\nDagger.Events.IDMetrics\nDagger.Events.TimelineMetrics\nDagger.Events.FullMetrics\nDagger.Events.BytesAllocd\nDagger.Events.CPULoadAverages\nDagger.Events.MemoryFree\nDagger.Events.EventSaturation\nDagger.Events.WorkerSaturation\nDagger.Events.ProcessorSaturation","category":"page"},{"location":"logging/#Dagger.Events.CoreMetrics","page":"Logging and Graphing","title":"Dagger.Events.CoreMetrics","text":"CoreMetrics\n\nTracks the timestamp, category, and kind of the Event object generated by log events.\n\n\n\n\n\n","category":"type"},{"location":"logging/#Dagger.Events.IDMetrics","page":"Logging and Graphing","title":"Dagger.Events.IDMetrics","text":"IDMetrics\n\nTracks the ID of Event objects generated by log events.\n\n\n\n\n\n","category":"type"},{"location":"logging/#Dagger.Events.TimelineMetrics","page":"Logging and Graphing","title":"Dagger.Events.TimelineMetrics","text":"TimelineMetrics\n\nTracks the timeline of Event objects generated by log events.\n\n\n\n\n\n","category":"type"},{"location":"logging/#Dagger.Events.FullMetrics","page":"Logging and Graphing","title":"Dagger.Events.FullMetrics","text":"FullMetrics\n\nTracks the full Event object generated by log events.\n\n\n\n\n\n","category":"type"},{"location":"logging/#Dagger.Events.BytesAllocd","page":"Logging and Graphing","title":"Dagger.Events.BytesAllocd","text":"BytesAllocd\n\nTracks memory allocated for Chunks.\n\n\n\n\n\n","category":"type"},{"location":"logging/#Dagger.Events.CPULoadAverages","page":"Logging and Graphing","title":"Dagger.Events.CPULoadAverages","text":"CPULoadAverages\n\nMonitors the CPU load averages.\n\n\n\n\n\n","category":"type"},{"location":"logging/#Dagger.Events.MemoryFree","page":"Logging and Graphing","title":"Dagger.Events.MemoryFree","text":"MemoryFree\n\nMonitors the percentage of free system memory.\n\n\n\n\n\n","category":"type"},{"location":"logging/#Dagger.Events.EventSaturation","page":"Logging and Graphing","title":"Dagger.Events.EventSaturation","text":"EventSaturation\n\nTracks the compute saturation (running tasks) per-processor.\n\n\n\n\n\n","category":"type"},{"location":"logging/#Dagger.Events.WorkerSaturation","page":"Logging and Graphing","title":"Dagger.Events.WorkerSaturation","text":"WorkerSaturation\n\nTracks the compute saturation (running tasks).\n\n\n\n\n\n","category":"type"},{"location":"logging/#Dagger.Events.ProcessorSaturation","page":"Logging and Graphing","title":"Dagger.Events.ProcessorSaturation","text":"ProcessorSaturation\n\nTracks the compute saturation (running tasks) per-processor.\n\n\n\n\n\n","category":"type"},{"location":"logging/","page":"Logging and Graphing","title":"Logging and Graphing","text":"The MultiEventLog also has a mechanism to call a set of functions, called \"aggregators\", after all consumers have been executed, and are passed the full set of log streams as a Dict{Symbol,Vector{Any}}. The only one currently shipped with Dagger directly is the LogWindow:","category":"page"},{"location":"logging/","page":"Logging and Graphing","title":"Logging and Graphing","text":"Dagger.Events.LogWindow\nDagger.Events.TableStorage","category":"page"},{"location":"logging/#Dagger.Events.LogWindow","page":"Logging and Graphing","title":"Dagger.Events.LogWindow","text":"LogWindow\n\nAggregator that prunes events to within a given time window.\n\n\n\n\n\n","category":"type"},{"location":"logging/#Dagger.Events.TableStorage","page":"Logging and Graphing","title":"Dagger.Events.TableStorage","text":"TableStorage\n\nLogWindow-compatible aggregator which stores logs in a Tables.jl-compatible sink.\n\nUsing a TableStorage is reasonably simple:\n\nml = Dagger.MultiEventLog()\n\n... # Add some events\n\nlw = Dagger.Events.LogWindow(5*10^9, :core)\n\n# Create a DataFrame with one Any[] for each event\ndf = DataFrame([key=>[] for key in keys(ml.consumers)]...)\n\n# Create the TableStorage and register its creation handler\nts = Dagger.Events.TableStorage(df)\npush!(lw.creation_handlers, ts)\n\nml.aggregators[:lw] = lw\n\n# Logs will now be saved into `df` automatically, and packages like\n# DaggerWebDash.jl will automatically use it to retrieve subsets of the logs.\n\n\n\n\n\n","category":"type"},{"location":"logging/#LocalEventLog","page":"Logging and Graphing","title":"LocalEventLog","text":"","category":"section"},{"location":"logging/","page":"Logging and Graphing","title":"Logging and Graphing","text":"The LocalEventLog is generally only useful when you want combined events (event start and finish combined as a single unit), and only care about a few simple built-in generated events. Let's attach one to our context:","category":"page"},{"location":"logging/","page":"Logging and Graphing","title":"Logging and Graphing","text":"ctx = Context()\nlog = Dagger.LocalEventLog()\nctx.log_sink = log","category":"page"},{"location":"logging/","page":"Logging and Graphing","title":"Logging and Graphing","text":"Now anytime ctx is used as the context for a scheduler, the scheduler will log events into log.","category":"page"},{"location":"logging/","page":"Logging and Graphing","title":"Logging and Graphing","text":"Once sufficient data has been accumulated into a LocalEventLog, it can be gathered to a single host via Dagger.get_logs!(log). The result is a Vector of Dagger.Timespan objects, which describe some metadata about an operation that occured and the scheduler logged. These events may be introspected directly, or may also be rendered to a DOT-format string:","category":"page"},{"location":"logging/","page":"Logging and Graphing","title":"Logging and Graphing","text":"logs = Dagger.get_logs!(log)\nstr = Dagger.show_plan(logs)","category":"page"},{"location":"logging/","page":"Logging and Graphing","title":"Logging and Graphing","text":"Dagger.show_plan can also be called as Dagger.show_plan(io::IO, logs) to write the graph to a file or other IO object. The string generated by this function may be passed to an external tool like Graphviz for rendering. Note that this method doesn't display input arguments to the DAG (non-Thunks); you can call Dagger.show_plan(logs, thunk), where thunk is the output Thunk of the DAG, to render argument nodes.","category":"page"},{"location":"logging/","page":"Logging and Graphing","title":"Logging and Graphing","text":"note: Note\nDagger.get_logs! clears out the event logs, so that old events don't mix with new ones from future DAGs.","category":"page"},{"location":"logging/","page":"Logging and Graphing","title":"Logging and Graphing","text":"As a convenience, it's possible to set ctx.log_file to the path to an output file, and then calls to compute(ctx, ...)/collect(ctx, ...) will automatically write the graph in DOT format to that path. There is also a benefit to this approach over manual calls to get_logs! and show_plan: DAGs which aren't Thunks (such as operations on the Dagger.DArray) will be properly rendered with input arguments (which normally aren't rendered because a Thunk is dynamically generated from such operations by Dagger before scheduling).","category":"page"},{"location":"logging/#FilterLog","page":"Logging and Graphing","title":"FilterLog","text":"","category":"section"},{"location":"logging/","page":"Logging and Graphing","title":"Logging and Graphing","text":"The FilterLog exists to allow writing events to a user-defined location (such as a database, file, or network socket). It is not currently tested or documented.","category":"page"},{"location":"api/types/","page":"Types","title":"Types","text":"CurrentModule = Dagger","category":"page"},{"location":"api/types/#Types","page":"Types","title":"Types","text":"","category":"section"},{"location":"api/types/","page":"Types","title":"Types","text":"Pages = [\"types.md\"]","category":"page"},{"location":"api/types/#General","page":"Types","title":"General","text":"","category":"section"},{"location":"api/types/","page":"Types","title":"Types","text":"Thunk\nEagerThunk\nChunk\nUnitDomain","category":"page"},{"location":"api/types/#Dagger.Thunk","page":"Types","title":"Dagger.Thunk","text":"Thunk\n\nWraps a callable object to be run with Dagger. A Thunk is typically created through a call to delayed or its macro equivalent @par.\n\nConstructors\n\ndelayed(f; kwargs...)(args...)\n@par [option=value]... f(args...)\n\nExamples\n\njulia> t = delayed(sin)(π)  # creates a Thunk to be computed later\nThunk(sin, (π,))\n\njulia> collect(t)  # computes the result and returns it to the current process\n1.2246467991473532e-16\n\nArguments\n\nf: The function to be called upon execution of the Thunk.\nargs: The arguments to be passed to the Thunk.\nkwargs: The properties describing unique behavior of this Thunk. Details\n\nfor each property are described in the next section.\n\noption=value: The same as passing kwargs to delayed.\n\nPublic Properties\n\nmeta::Bool=false: If true, instead of fetching cached arguments from\n\nChunks and passing the raw arguments to f, instead pass the Chunk. Useful for doing manual fetching or manipulation of Chunk references. Non-Chunk arguments are still passed as-is.\n\nprocessor::Processor=OSProc() - The processor associated with f. Useful if\n\nf is a callable struct that exists on a given processor and should be transferred appropriately.\n\nscope::Dagger.AbstractScope=AnyScope() - The scope associated with f.\n\nUseful if f is a function or callable struct that may only be transferred to, and executed within, the specified scope.\n\nOptions\n\noptions: A Sch.ThunkOptions struct providing the options for the Thunk.\n\nIf omitted, options can also be specified by passing key-value pairs as kwargs.\n\n\n\n\n\n","category":"type"},{"location":"api/types/#Dagger.EagerThunk","page":"Types","title":"Dagger.EagerThunk","text":"EagerThunk\n\nReturned from spawn/@spawn calls. Represents a task that is in the scheduler, potentially ready to execute, executing, or finished executing. May be fetch'd or wait'd on at any time.\n\n\n\n\n\n","category":"type"},{"location":"api/types/#Dagger.Chunk","page":"Types","title":"Dagger.Chunk","text":"Chunk\n\nA reference to a piece of data located on a remote worker. Chunks are typically created with Dagger.tochunk(data), and the data can then be accessed from any worker with collect(::Chunk). Chunks are serialization-safe, and use distributed refcounting (provided by MemPool.DRef) to ensure that the data referenced by a Chunk won't be GC'd, as long as a reference exists on some worker.\n\nEach Chunk is associated with a given Dagger.Processor, which is (in a sense) the processor that \"owns\" or contains the data. Calling collect(::Chunk) will perform data movement and conversions defined by that processor to safely serialize the data to the calling worker.\n\nConstructors\n\nSee tochunk.\n\n\n\n\n\n","category":"type"},{"location":"api/types/#Dagger.UnitDomain","page":"Types","title":"Dagger.UnitDomain","text":"UnitDomain\n\nDefault domain – has no information about the value\n\n\n\n\n\n","category":"type"},{"location":"api/types/#Processors","page":"Types","title":"Processors","text":"","category":"section"},{"location":"api/types/","page":"Types","title":"Types","text":"Processor\nOSProc\nThreadProc","category":"page"},{"location":"api/types/#Dagger.Processor","page":"Types","title":"Dagger.Processor","text":"Processor\n\nAn abstract type representing a processing device and associated memory, where data can be stored and operated on. Subtypes should be immutable, and instances should compare equal if they represent the same logical processing device/memory. Subtype instances should be serializable between different nodes. Subtype instances may contain a pointer to a \"parent\" Processor to make it easy to transfer data to/from other types of Processor at runtime.\n\n\n\n\n\n","category":"type"},{"location":"api/types/#Dagger.OSProc","page":"Types","title":"Dagger.OSProc","text":"OSProc <: Processor\n\nJulia CPU (OS) process, identified by Distributed pid. The logical parent of all processors on a given node, but otherwise does not participate in computations.\n\n\n\n\n\n","category":"type"},{"location":"api/types/#Dagger.ThreadProc","page":"Types","title":"Dagger.ThreadProc","text":"ThreadProc <: Processor\n\nJulia CPU (OS) thread, identified by Julia thread ID.\n\n\n\n\n\n","category":"type"},{"location":"api/types/#Context","page":"Types","title":"Context","text":"","category":"section"},{"location":"api/types/","page":"Types","title":"Types","text":"Context","category":"page"},{"location":"api/types/#Dagger.Context","page":"Types","title":"Dagger.Context","text":"Context(;nthreads=Threads.nthreads()) -> Context\nContext(xs::Vector{OSProc}) -> Context\nContext(xs::Vector{Int}) -> Context\n\nCreate a Context, by default adding each available worker once from every available thread. Use the nthreads keyword to use a different number of threads.\n\nIt is also possible to create a Context from a vector of OSProc, or equivalently the underlying process ids can also be passed directly as a Vector{Int}.\n\nSpecial fields include:\n\n'log_sink': A log sink object to use, if any.\nlog_file::Union{String,Nothing}: Path to logfile. If specified, at\n\nscheduler termination logs will be collected, combined with input thunks, and written out in DOT format to this location.\n\nprofile::Bool: Whether or not to perform profiling with Profile stdlib.\n\n\n\n\n\n","category":"type"},{"location":"api/types/#Logging","page":"Types","title":"Logging","text":"","category":"section"},{"location":"api/types/","page":"Types","title":"Types","text":"NoOpLog\nLocalEventLog","category":"page"},{"location":"api/types/#Dagger.NoOpLog","page":"Types","title":"Dagger.NoOpLog","text":"NoOpLog\n\nDisables event logging entirely.\n\n\n\n\n\n","category":"type"},{"location":"api/types/#Dagger.LocalEventLog","page":"Types","title":"Dagger.LocalEventLog","text":"LocalEventLog\n\nStores events in a process-local array. Accessing the logs is all-or-nothing; if multiple consumers call get_logs!, they will get different sets of logs.\n\n\n\n\n\n","category":"type"},{"location":"api/types/#Scheduling","page":"Types","title":"Scheduling","text":"","category":"section"},{"location":"api/types/","page":"Types","title":"Types","text":"Sch.SchedulerOptions\nSch.ThunkOptions\nSch.MaxUtilization\nSch.DynamicThunkException","category":"page"},{"location":"api/types/#Dagger.Sch.SchedulerOptions","page":"Types","title":"Dagger.Sch.SchedulerOptions","text":"SchedulerOptions\n\nStores DAG-global options to be passed to the Dagger.Sch scheduler.\n\nArguments\n\nsingle::Int=0: Force all work onto worker with specified id. 0 disables\n\nthis option.\n\nproclist=nothing: Force scheduler to use one or more processors that are\n\ninstances/subtypes of a contained type. Alternatively, a function can be supplied, and the function will be called with a processor as the sole argument and should return a Bool result to indicate whether or not to use the given processor. nothing enables all default processors.\n\nallow_errors::Bool=true: Allow thunks to error without affecting\n\nnon-dependent thunks.\n\ncheckpoint=nothing: If not nothing, uses the provided function to save\n\nthe final result of the current scheduler invocation to persistent storage, for later retrieval by restore.\n\nrestore=nothing: If not nothing, uses the provided function to return the\n\n(cached) final result of the current scheduler invocation, were it to execute. If this returns a Chunk, all thunks will be skipped, and the Chunk will be returned.  If nothing is returned, restoring is skipped, and the scheduler will execute as usual. If this function throws an error, restoring will be skipped, and the error will be displayed.\n\nround_robin::Bool=false: Whether to schedule in round-robin mode, which\n\nspreads load instead of the default behavior of filling processors to capacity.\n\n\n\n\n\n","category":"type"},{"location":"api/types/#Dagger.Sch.ThunkOptions","page":"Types","title":"Dagger.Sch.ThunkOptions","text":"ThunkOptions\n\nStores Thunk-local options to be passed to the Dagger.Sch scheduler.\n\nArguments\n\nsingle::Int=0: Force thunk onto worker with specified id. 0 disables this\n\noption.\n\nproclist=nothing: Force thunk to use one or more processors that are\n\ninstances/subtypes of a contained type. Alternatively, a function can be supplied, and the function will be called with a processor as the sole argument and should return a Bool result to indicate whether or not to use the given processor. nothing enables all default processors.\n\nprocutil::Dict{Type,Any}=Dict{Type,Any}(): Indicates the maximum expected\n\nprocessor utilization for this thunk. Each keypair maps a processor type to the utilization, where the value can be a real (approx. the number of processors of this type utilized), or MaxUtilization() (utilizes all processors of this type). By default, the scheduler assumes that this thunk only uses one processor.\n\nallow_errors::Bool=true: Allow this thunk to error without affecting\n\nnon-dependent thunks.\n\ncheckpoint=nothing: If not nothing, uses the provided function to save\n\nthe result of the thunk to persistent storage, for later retrieval by restore.\n\nrestore=nothing: If not nothing, uses the provided function to return the\n\n(cached) result of this thunk, were it to execute.  If this returns a Chunk, this thunk will be skipped, and its result will be set to the Chunk.  If nothing is returned, restoring is skipped, and the thunk will execute as usual. If this function throws an error, restoring will be skipped, and the error will be displayed.\n\n\n\n\n\n","category":"type"},{"location":"api/types/#Dagger.Sch.MaxUtilization","page":"Types","title":"Dagger.Sch.MaxUtilization","text":"MaxUtilization\n\nIndicates a thunk that uses all processors of a given type.\n\n\n\n\n\n","category":"type"},{"location":"api/types/#Dagger.Sch.DynamicThunkException","page":"Types","title":"Dagger.Sch.DynamicThunkException","text":"Thrown when a dynamic thunk encounters an exception in Dagger's utilities.\n\n\n\n\n\n","category":"type"},{"location":"api/types/#Arrays","page":"Types","title":"Arrays","text":"","category":"section"},{"location":"api/types/","page":"Types","title":"Types","text":"DArray\nBlocks\nArrayDomain","category":"page"},{"location":"api/types/#Dagger.DArray","page":"Types","title":"Dagger.DArray","text":"DArray{T,N,F}(domain, subdomains, chunks, concat)\nDArray(T, domain, subdomains, chunks, [concat=cat])\n\nAn N-dimensional distributed array of element type T, with a concatenation function of type F.\n\nArguments\n\nT: element type\ndomain::ArrayDomain{N}: the whole ArrayDomain of the array\nsubdomains::AbstractArray{ArrayDomain{N}, N}: a DomainBlocks of the same dimensions as the array\nchunks::AbstractArray{Union{Chunk,Thunk}, N}: an array of chunks of dimension N\nconcat::F: a function of type F. concat(x, y; dims=d) takes two chunks x and y and concatenates them along dimension d. cat is used by default.\n\n\n\n\n\n","category":"type"},{"location":"api/types/#Dagger.Blocks","page":"Types","title":"Dagger.Blocks","text":"Blocks(xs...)\n\nIndicates the size of an array operation, specified as xs, whose length indicates the number of dimensions in the resulting array.\n\n\n\n\n\n","category":"type"},{"location":"api/types/#Dagger.ArrayDomain","page":"Types","title":"Dagger.ArrayDomain","text":"ArrayDomain{N}\n\nAn N-dimensional domain over an array.\n\n\n\n\n\n","category":"type"},{"location":"api/types/#File-IO","page":"Types","title":"File IO","text":"","category":"section"},{"location":"api/types/","page":"Types","title":"Types","text":"warning: Warning\nThese APIs are currently untested and may be removed or modified.","category":"page"},{"location":"api/types/","page":"Types","title":"Types","text":"FileReader","category":"page"},{"location":"api/types/#Dagger.FileReader","page":"Types","title":"Dagger.FileReader","text":"FileReader\n\nUsed as a Chunk handle for reading a file, starting at a given offset.\n\n\n\n\n\n","category":"type"},{"location":"scopes/#Scopes","page":"Scopes","title":"Scopes","text":"","category":"section"},{"location":"scopes/","page":"Scopes","title":"Scopes","text":"Sometimes you will have data that is only meaningful in a certain location, such as within a single Julia process, a given server, or even for a specific Dagger processor. We call this location a \"scope\" in Dagger, denoting the bounds within which the data is meaningful and valid. For example, C pointers are typically scoped to a process, file paths are scoped to one or more servers dependent on filesystem configuration, etc. By default, Dagger doesn't recognize this; it treats everything passed into a task, or generated from a task, as inherently safe to transfer anywhere else. When this is not the case, Dagger provides optional scopes to instruct the scheduler where data is considered valid.","category":"page"},{"location":"scopes/#Scope-Basics","page":"Scopes","title":"Scope Basics","text":"","category":"section"},{"location":"scopes/","page":"Scopes","title":"Scopes","text":"Let's take the example of a webcam handle generated by VideoIO.jl. This handle is a C pointer, and thus has process scope. We can open the handle on a given process, and set the scope of the resulting data to a ProcessScope(), which defaults to the current Julia process:","category":"page"},{"location":"scopes/","page":"Scopes","title":"Scopes","text":"using VideoIO\n\nfunction get_handle()\n    handle = VideoIO.opencamera()\n    proc = Dagger.thunk_processor()\n    scope = ProcessScope()\n    return Dagger.tochunk(handle, proc, scope)\nend\n\ncam_handle = Dagger.@spawn get_handle()","category":"page"},{"location":"scopes/","page":"Scopes","title":"Scopes","text":"Now, wherever cam_handle is passed, Dagger will ensure that any computations on the handle only happen within its defined scope. For example, we can read from the camera:","category":"page"},{"location":"scopes/","page":"Scopes","title":"Scopes","text":"cam_frame = Dagger.@spawn read(cam_handle)","category":"page"},{"location":"scopes/","page":"Scopes","title":"Scopes","text":"The cam_frame task is executed within any processor on the same process that the cam_handle task was executed on. Of course, the resulting camera frame is not scoped to anywhere specific (denoted as AnyScope()), and thus computations on it may execute anywhere.","category":"page"},{"location":"scopes/","page":"Scopes","title":"Scopes","text":"You may also encounter situations where you want to use a callable struct (such as a closure, or a Flux.jl layer) only within a certain scope; you can specify the scope of the function pretty easily:","category":"page"},{"location":"scopes/","page":"Scopes","title":"Scopes","text":"using Flux\nm = Chain(...)\n# If `m` is only safe to transfer to and execute on this process,\n# we can set a `ProcessScope` on it:\nresult = Dagger.@spawn scope=ProcessScope() m(rand(8,8))","category":"page"},{"location":"scopes/","page":"Scopes","title":"Scopes","text":"Setting a scope on the function treats it as a regular piece of data (like the arguments to the function), so it participates in the scoping rules described in the following sections all the same.","category":"page"},{"location":"scopes/","page":"Scopes","title":"Scopes","text":"Now, let's try out some other kinds of scopes, starting with NodeScope. This scope encompasses the server that one or more Julia processes may be running on. Say we want to use memory mapping (mmap) to more efficiently send arrays between two tasks. We can construct the mmap'd array in one task, attach a NodeScope() to it, and using the path of the mmap'd file to communicate its location, lock downstream tasks to the same server:","category":"page"},{"location":"scopes/","page":"Scopes","title":"Scopes","text":"using Mmap\n\nfunction generate()\n    path = \"myfile.bin\"\n    arr = Mmap.mmap(path, Matrix{Int}, (64,64))\n    fill!(arr, 1)\n    Mmap.sync!(arr)\n    Dagger.tochunk(path, Dagger.thunk_processor(), NodeScope())\nend\n\nfunction consume(path)\n    arr = Mmap.mmap(path, Matrix{Int}, (64,64))\n    sum(arr)\nend\n\na = Dagger.@spawn generate()\n@assert fetch(Dagger.@spawn consume(a)) == 64*64","category":"page"},{"location":"scopes/","page":"Scopes","title":"Scopes","text":"Whatever server a executed on, b will also execute on!","category":"page"},{"location":"scopes/","page":"Scopes","title":"Scopes","text":"Finally, we come to the \"lowest\" scope on the scope hierarchy, the ExactScope. This scope specifies one exact processor as the bounding scope, and is typically useful in certain limited cases. We won't provide an example here, because you don't usually need to ever use this scope, but if you already understand the NodeScope and ProcessScope, the ExactScope should be easy to figure out.","category":"page"},{"location":"scopes/#Union-Scopes","page":"Scopes","title":"Union Scopes","text":"","category":"section"},{"location":"scopes/","page":"Scopes","title":"Scopes","text":"Sometimes one simple scope isn't enough! In that case, you can use the UnionScope to construct the union of two or more scopes. Say, for example, you have some sensitive data on your company's servers that you want to compute summaries of, but you'll be driving the computation from your laptop, and you aren't allowed to send the data itself outside of the company's network. You could accomplish this by constructing a UnionScope of ProcessScopes of each of the non-laptop Julia processes, and use that to ensure that the data in its original form always stays within the company network:","category":"page"},{"location":"scopes/","page":"Scopes","title":"Scopes","text":"addprocs(4) # some local processors\nprocs = addprocs([(\"server.company.com\", 4)]) # some company processors\n\nsecrets_scope = UnionScope(ProcessScope.(procs))\n\nfunction generate_secrets()\n    secrets = open(\"/shared/secret_results.txt\", \"r\") do io\n        String(read(io))\n    end\n    Dagger.tochunk(secrets, Dagger.thunk_processor(), secrets_scope)\nend\n\nsummarize(secrets) = occursin(\"QA Pass\", secrets)\n\n# Generate the data on the first company process\nsensitive_data = Dagger.@spawn single=first(procs) generate_secrets()\n\n# We can safely call this, knowing that it will be executed on a company server\nqa_passed = Dagger.@spawn summarize(sensitive_data)","category":"page"},{"location":"scopes/#Mismatched-Scopes","page":"Scopes","title":"Mismatched Scopes","text":"","category":"section"},{"location":"scopes/","page":"Scopes","title":"Scopes","text":"You might now be thinking, \"What if I want to run a task on multiple pieces of data whose scopes don't match up?\" In such a case, Dagger will throw an error, refusing to schedule that task, since the intersection of the data scopes is an empty set (there is no feasible processor which can satisfy the scoping constraints). For example:","category":"page"},{"location":"scopes/","page":"Scopes","title":"Scopes","text":"ps2 = ProcessScope(2)\nps3 = ProcessScope(3)\n\ngenerate(scope) = Dagger.tochunk(rand(64), Dagger.thunk_processor(), scope)\n\nd2 = Dagger.@spawn generate(ps2) # Run on process 2\nd3 = Dagger.@spawn generate(ps3) # Run on process 3\nres = Dagger.@spawn d2 * d3 # An error!","category":"page"},{"location":"scopes/","page":"Scopes","title":"Scopes","text":"Moral of the story: only use scopes when you know you really need them, and if you aren't careful to arrange everything just right, be prepared for Dagger to refuse to schedule your tasks! Scopes should only be used to ensure correctness of your programs, and are not intended to be used to optimize the schedule that Dagger uses for your tasks, since restricting the scope of execution for tasks will necessarily reduce the optimizations that Dagger's scheduler can perform.","category":"page"},{"location":"dtable/#Distributed-table","page":"Distributed Table","title":"Distributed table","text":"","category":"section"},{"location":"dtable/","page":"Distributed Table","title":"Distributed Table","text":"The DTable, or \"distributed table\", is an abstraction layer on top of Dagger that allows loading table-like structures into a distributed environment.  The main idea is that a Tables.jl-compatible source provided by the user gets partitioned into several parts and stored as Chunks.  These can then be distributed across worker processes by the scheduler as operations are performed on the containing DTable.","category":"page"},{"location":"dtable/","page":"Distributed Table","title":"Distributed Table","text":"Operations performed on a DTable leverage the fact that the table is partitioned, and will try to apply functions per-partition first, afterwards merging the results if needed.","category":"page"},{"location":"dtable/","page":"Distributed Table","title":"Distributed Table","text":"The distributed table is backed by Dagger's Eager API (Dagger.@spawn and Dagger.spawn).  To provide a familiar usage pattern you can call fetch on a DTable instance, which returns an in-memory instance of the underlying table type (such as a DataFrame, TypedTable, etc).","category":"page"},{"location":"dtable/#Creating-a-DTable","page":"Distributed Table","title":"Creating a DTable","text":"","category":"section"},{"location":"dtable/","page":"Distributed Table","title":"Distributed Table","text":"There are currently two ways of constructing a distributed table:","category":"page"},{"location":"dtable/#Tables.jl-source","page":"Distributed Table","title":"Tables.jl source","text":"","category":"section"},{"location":"dtable/","page":"Distributed Table","title":"Distributed Table","text":"Provide a Tables.jl compatible source, as well as a chunksize, which is the maximum number of rows of each partition:","category":"page"},{"location":"dtable/","page":"Distributed Table","title":"Distributed Table","text":"julia> using Dagger\n\njulia> table = (a=[1, 2, 3, 4, 5], b=[6, 7, 8, 9, 10]);\n\njulia> d = DTable(table, 2)\nDTable with 3 partitions\nTabletype: NamedTuple\n\njulia> fetch(d)\n(a = [1, 2, 3, 4, 5], b = [6, 7, 8, 9, 10])","category":"page"},{"location":"dtable/#Loader-function-and-file-list","page":"Distributed Table","title":"Loader function and file list","text":"","category":"section"},{"location":"dtable/","page":"Distributed Table","title":"Distributed Table","text":"Provide a loader_function and a list of filenames, which are parts of the full table:","category":"page"},{"location":"dtable/","page":"Distributed Table","title":"Distributed Table","text":"julia> using Dagger, CSV\n\njulia> files = [\"1.csv\", \"2.csv\", \"3.csv\"];\n\njulia> d = DTable(CSV.File, files)\nDTable with 3 partitions\nTabletype: unknown (use `tabletype!(::DTable)`)\n\njulia> tabletype(d)\nNamedTuple\n\njulia> fetch(d)\n(a = [1, 2, 1, 2, 1, 2], b = [6, 7, 6, 7, 6, 7])","category":"page"},{"location":"dtable/#Underlying-table-type","page":"Distributed Table","title":"Underlying table type","text":"","category":"section"},{"location":"dtable/","page":"Distributed Table","title":"Distributed Table","text":"The underlying type of the partition is, by default, of the type constructed by Tables.materializer(source):","category":"page"},{"location":"dtable/","page":"Distributed Table","title":"Distributed Table","text":"julia> table = (a=[1, 2, 3, 4, 5], b=[6, 7, 8, 9, 10]);\n\njulia> d = DTable(table, 2)\nDTable with 3 partitions\nTabletype: NamedTuple\n\njulia> fetch(d)\n(a = [1, 2, 3, 4, 5], b = [6, 7, 8, 9, 10])","category":"page"},{"location":"dtable/","page":"Distributed Table","title":"Distributed Table","text":"To override the underlying type you can provide a kwarg tabletype to the DTable constructor.  You can also choose which tabletype the DTable should be fetched into:","category":"page"},{"location":"dtable/","page":"Distributed Table","title":"Distributed Table","text":"julia> using DataFrames\n\njulia> table = (a=[1, 2, 3, 4, 5], b=[6, 7, 8, 9, 10]);\n\njulia> d = DTable(table, 2; tabletype=DataFrame)\nDTable with 3 partitions\nTabletype: DataFrame\n\njulia> fetch(d)\n5×2 DataFrame\n Row │ a      b\n     │ Int64  Int64\n─────┼──────────────\n   1 │     1      6\n   2 │     2      7\n   3 │     3      8\n   4 │     4      9\n   5 │     5     10\n\njulia> fetch(d, NamedTuple)\n(a = [1, 2, 3, 4, 5], b = [6, 7, 8, 9, 10])","category":"page"},{"location":"dtable/#Table-operations","page":"Distributed Table","title":"Table operations","text":"","category":"section"},{"location":"dtable/","page":"Distributed Table","title":"Distributed Table","text":"Warning: this interface is experimental and may change at any time","category":"page"},{"location":"dtable/","page":"Distributed Table","title":"Distributed Table","text":"The current set of operations available consist of three simple functions: map, filter and reduce.","category":"page"},{"location":"dtable/","page":"Distributed Table","title":"Distributed Table","text":"Below is an example of their usage.","category":"page"},{"location":"dtable/","page":"Distributed Table","title":"Distributed Table","text":"For more information please refer to the API documentation and unit tests.","category":"page"},{"location":"dtable/","page":"Distributed Table","title":"Distributed Table","text":"julia> using Dagger\n\njulia> d = DTable((k = repeat(['a', 'b'], 500), v = repeat(1:10, 100)), 100)\nDTable with 10 partitions\nTabletype: NamedTuple\n\njulia> using DataFrames\n\njulia> m = map(x -> (t = x.k + x.v, v = x.v), d)\nDTable with 10 partitions\nTabletype: NamedTuple\n\njulia> fetch(m, DataFrame)\n1000×2 DataFrame\n  Row │ t     v\n      │ Char  Int64\n──────┼─────────────\n    1 │ b         1\n    2 │ d         2\n    3 │ d         3\n  ⋮   │  ⋮      ⋮\n  999 │ j         9\n 1000 │ l        10\n    995 rows omitted\n\njulia> f = filter(x -> x.t == 'd', m)\nDTable with 10 partitions\nTabletype: NamedTuple\n\njulia> fetch(f, DataFrame)\n200×2 DataFrame\n Row │ t     v\n     │ Char  Int64\n─────┼─────────────\n   1 │ d         2\n   2 │ d         3\n  ⋮  │  ⋮      ⋮\n 200 │ d         3\n   197 rows omitted\n\njulia> r = reduce(+, m, cols=[:v])\nEagerThunk (running)\n\njulia> fetch(r)\n(v = 5500,)","category":"page"},{"location":"dtable/#Dagger.groupby-interface","page":"Distributed Table","title":"Dagger.groupby interface","text":"","category":"section"},{"location":"dtable/","page":"Distributed Table","title":"Distributed Table","text":"A DTable can be grouped which will result in creation of a GDTable. A distinct set of values contained in a single or multiple columns can be used as grouping keys. If a transformation of a row needs to be performed in order to obtain the grouping key there's also an option to provide a custom function returning a key, which is applied per row.","category":"page"},{"location":"dtable/","page":"Distributed Table","title":"Distributed Table","text":"The set of keys the GDTable is grouped by can be obtained using the keys(gd::GDTable) function. To get a fragment of the GDTable containing records belonging under a single key the getindex(gd::GDTable, key) function can be used.","category":"page"},{"location":"dtable/","page":"Distributed Table","title":"Distributed Table","text":"julia> d = DTable((a=shuffle(repeat('a':'d', inner=4, outer=4)),b=repeat(1:4, 16)), 4)\nDTable with 16 partitions\nTabletype: NamedTuple\n\njulia> Dagger.groupby(d, :a)\nGDTable with 4 partitions and 4 keys\nTabletype: NamedTuple\nGrouped by: [:a]\n\njulia> Dagger.groupby(d, [:a, :b])\nGDTable with 16 partitions and 16 keys\nTabletype: NamedTuple\nGrouped by: [:a, :b]\n\njulia> Dagger.groupby(d, row -> row.a + row.b)\nGDTable with 7 partitions and 7 keys\nTabletype: NamedTuple\nGrouped by: #5\n\njulia> g = Dagger.groupby(d, :a); keys(g)\nKeySet for a Dict{Char, Vector{UInt64}} with 4 entries. Keys:\n  'c'\n  'd'\n  'a'\n  'b'\n\njulia> g['c']\nDTable with 1 partitions\nTabletype: NamedTuple","category":"page"},{"location":"dtable/#GDTable-operations","page":"Distributed Table","title":"GDTable operations","text":"","category":"section"},{"location":"dtable/","page":"Distributed Table","title":"Distributed Table","text":"Operations such as map, filter, reduce can be performed on a GDTable","category":"page"},{"location":"dtable/","page":"Distributed Table","title":"Distributed Table","text":"julia> g = Dagger.groupby(d, [:a, :b])\nGDTable with 16 partitions and 16 keys\nTabletype: NamedTuple\nGrouped by: [:a, :b]\n\njulia> f = filter(x -> x.a != 'd', g)\nGDTable with 16 partitions and 16 keys\nTabletype: NamedTuple\nGrouped by: [:a, :b]\n\njulia> trim!(f)\nGDTable with 12 partitions and 12 keys\nTabletype: NamedTuple\nGrouped by: [:a, :b]\n\njulia> m = map(r -> (a = r.a, b = r.b, c = r.b .- 3), f)\nGDTable with 12 partitions and 12 keys\nTabletype: NamedTuple\nGrouped by: [:a, :b]\n\njulia> r = reduce(*, m)\nEagerThunk (running)\n\njulia> DataFrame(fetch(r))\n12×5 DataFrame\n Row │ a     b      result_a  result_b  result_c \n     │ Char  Int64  String    Int64     Int64    \n─────┼───────────────────────────────────────────\n   1 │ a         1  aaaa             1        16\n   2 │ c         3  ccc             27         0\n   3 │ a         3  aa               9         0\n   4 │ b         4  bbbb           256         1\n   5 │ c         4  cccc           256         1\n   6 │ b         2  bbbb            16         1\n   7 │ b         1  bbbb             1        16\n   8 │ a         2  aaa              8        -1\n   9 │ a         4  aaaaaaa      16384         1\n  10 │ b         3  bbbb            81         0\n  11 │ c         2  ccccc           32        -1\n  12 │ c         1  cccc             1        16","category":"page"},{"location":"dtable/#Iterating-over-a-GDTable","page":"Distributed Table","title":"Iterating over a GDTable","text":"","category":"section"},{"location":"dtable/","page":"Distributed Table","title":"Distributed Table","text":"GDTable can be iterated over and each element returned will be a pair of key and a DTable containing all rows associated with that grouping key.","category":"page"},{"location":"dtable/","page":"Distributed Table","title":"Distributed Table","text":"julia> d = DTable((a=repeat('a':'b', inner=2),b=1:4), 2)\nDTable with 2 partitions\nTabletype: NamedTuple\n\njulia> g = Dagger.groupby(d, :a)\nGDTable with 2 partitions and 2 keys\nTabletype: NamedTuple\nGrouped by: [:a]\n\njulia> for (key, dt) in g\n           println(\"Key: $key\")\n           println(fetch(dt, DataFrame))\n       end\nKey: a\n2×2 DataFrame\n Row │ a     b     \n     │ Char  Int64 \n─────┼─────────────\n   1 │ a         1\n   2 │ a         2\nKey: b\n2×2 DataFrame\n Row │ a     b     \n     │ Char  Int64 \n─────┼─────────────\n   1 │ b         3\n   2 │ b         4","category":"page"},{"location":"dtable/#API","page":"Distributed Table","title":"API","text":"","category":"section"},{"location":"dtable/","page":"Distributed Table","title":"Distributed Table","text":"DTable\ntabletype\ntabletype!\ntrim\ntrim!\nmap\nfilter\nreduce\ngroupby\nkeys\ngetindex","category":"page"},{"location":"dtable/#Dagger.DTable","page":"Distributed Table","title":"Dagger.DTable","text":"DTable\n\nStructure representing the distributed table based on Dagger.\n\nThe table is stored as a vector of Chunk structures which hold partitions of the table. That vector can also store EagerThunk structures when an operation that modifies the underlying partitions was applied to it (currently only filter).\n\n\n\n\n\n","category":"type"},{"location":"dtable/#Dagger.tabletype","page":"Distributed Table","title":"Dagger.tabletype","text":"tabletype(d::DTable)\n\nProvides the type of the underlying table partition. Uses the cached tabletype if available.\n\nIn case the tabletype cannot be obtained the default return value is NamedTuple.\n\n\n\n\n\ntabletype(gd::GDTable)\n\nProvides the type of the underlying table partition. Uses the cached tabletype if available.\n\nIn case the tabletype cannot be obtained the default return value is NamedTuple.\n\n\n\n\n\n","category":"function"},{"location":"dtable/#Dagger.tabletype!","page":"Distributed Table","title":"Dagger.tabletype!","text":"tabletype!(d::DTable)\n\nProvides the type of the underlying table partition and caches it in d.\n\nIn case the tabletype cannot be obtained the default return value is NamedTuple.\n\n\n\n\n\ntabletype!(gd::GDTable)\n\nProvides the type of the underlying table partition and caches it in gd.\n\nIn case the tabletype cannot be obtained the default return value is NamedTuple.\n\n\n\n\n\n","category":"function"},{"location":"dtable/#Dagger.trim","page":"Distributed Table","title":"Dagger.trim","text":"trim(d::DTable) -> DTable\n\nReturns d with empty chunks removed.\n\n\n\n\n\ntrim(gd::GDTable) -> GDTable\n\nReturns gd with empty chunks and keys removed.\n\n\n\n\n\n","category":"function"},{"location":"dtable/#Dagger.trim!","page":"Distributed Table","title":"Dagger.trim!","text":"trim!(d::DTable) -> DTable\n\nRemoves empty chunks from d.\n\n\n\n\n\ntrim!(gd::GDTable) -> GDTable\n\nRemoves empty chunks from gd and unused keys from its index.\n\n\n\n\n\n","category":"function"},{"location":"dtable/#Base.map","page":"Distributed Table","title":"Base.map","text":"map(f, d::DTable) -> DTable\n\nApplies f to each row of d. The applied function needs to return a Tables.Row compatible object (e.g. NamedTuple).\n\nExamples\n\njulia> d = DTable((a = [1, 2, 3], b = [1, 1, 1]), 2);\n\njulia> m = map(x -> (r = x.a + x.b,), d)\nDTable with 2 partitions\nTabletype: NamedTuple\n\njulia> fetch(m)\n(r = [2, 3, 4],)\n\njulia> m = map(x -> (r1 = x.a + x.b, r2 = x.a - x.b), d)\nDTable with 2 partitions\nTabletype: NamedTuple\n\njulia> fetch(m)\n(r1 = [2, 3, 4], r2 = [0, 1, 2])\n\n\n\n\n\nmap(f, gd::GDTable) -> GDTable\n\nApplies f to each row of gd. The applied function needs to return a Tables.Row compatible object (e.g. NamedTuple).\n\nExamples\n\njulia> g = Dagger.groupby(DTable((a=repeat('a':'c', inner=2),b=1:6), 2), :a)\nGDTable with 3 partitions and 3 keys\nTabletype: NamedTuple\nGrouped by: [:a]\n\njulia> m = map(r -> (a = r.a, b = r.b, c = r.a + r.b), g)\nGDTable with 3 partitions and 3 keys\nTabletype: NamedTuple\nGrouped by: [:a]\n\njulia> fetch(m)\n(a = ['a', 'a', 'c', 'c', 'b', 'b'], b = [1, 2, 5, 6, 3, 4], c = ['b', 'c', 'h', 'i', 'e', 'f'])\n\n\n\n\n\n","category":"function"},{"location":"dtable/#Base.filter","page":"Distributed Table","title":"Base.filter","text":"filter(f, d::DTable) -> DTable\n\nFilter d using f. Returns a filtered DTable that can be processed further.\n\nExamples\n\njulia> d = DTable((a = [1, 2, 3], b = [1, 1, 1]), 2);\n\njulia> f = filter(x -> x.a < 3, d)\nDTable with 2 partitions\nTabletype: NamedTuple\n\njulia> fetch(f)\n(a = [1, 2], b = [1, 1])\n\njulia> f = filter(x -> (x.a < 3) .& (x.b > 0), d)\nDTable with 2 partitions\nTabletype: NamedTuple\n\njulia> fetch(f)\n(a = [1, 2], b = [1, 1])\n\n\n\n\n\nfilter(f, gd::GDTable) -> GDTable\n\nFilter 'gd' using 'f', returning a filtered GDTable. Calling trim! on a filtered GDTable will clean up the empty keys and partitions.\n\nExamples\n\njulia> g = Dagger.groupby(DTable((a=repeat('a':'d', inner=2),b=1:8), 2), :a)\nGDTable with 4 partitions and 4 keys\nTabletype: NamedTuple\nGrouped by: [:a]\n\njulia> f = filter(x -> x.a ∈ ['a', 'b'], g)\nGDTable with 4 partitions and 4 keys\nTabletype: NamedTuple\nGrouped by: [:a]\n\njulia> fetch(f)\n(a = ['a', 'a', 'b', 'b'], b = [1, 2, 3, 4])\n\njulia> trim!(f)\nGDTable with 2 partitions and 2 keys\nTabletype: NamedTuple\nGrouped by: [:a]\n\n\n\n\n\n","category":"function"},{"location":"dtable/#Base.reduce","page":"Distributed Table","title":"Base.reduce","text":"reduce(f, d::DTable; cols=nothing, [init]) -> NamedTuple\n\nReduces d using function f applied on all columns of the DTable.\n\nBy providing the kwarg cols as a Vector{Symbol} object it's possible to restrict the reduction to the specified columns. The reduced values are provided in a NamedTuple under names of reduced columns.\n\nFor the init kwarg please refer to Base.reduce documentation, as it follows the same principles. \n\nExamples\n\njulia> d = DTable((a = [1, 2, 3], b = [1, 1, 1]), 2);\n\njulia> r1 = reduce(+, d)\nEagerThunk (running)\n\njulia> fetch(r1)\n(a = 6, b = 3)\n\njulia> r2 = reduce(*, d, cols=[:a])\nEagerThunk (running)\n\njulia> fetch(r2)\n(a = 6,)\n\n\n\n\n\nreduce(f, gd::GDTable; cols=nothing, prefix=\"result_\", [init]) -> EagerThunk -> NamedTuple\n\nReduces gd using function f applied on all columns of the DTable. Returns results per group in columns with names prefixed with the prefix kwarg. For more information on kwargs see reduce(f, d::DTable)\n\nExamples\n\njulia> g = Dagger.groupby(DTable((a=repeat('a':'d', inner=2),b=1:8), 2), :a)\nGDTable with 4 partitions and 4 keys\nTabletype: NamedTuple\nGrouped by: [:a]\n\njulia> fetch(reduce(*, g))\n(a = ['a', 'c', 'd', 'b'], result_a = [\"aa\", \"cc\", \"dd\", \"bb\"], result_b = [2, 30, 56, 12])\n\n\n\n\n\n","category":"function"},{"location":"dtable/#Base.keys","page":"Distributed Table","title":"Base.keys","text":"keys(gd::GDTable) -> KeySet\n\nReturns the keys that gd is grouped by.\n\n\n\n\n\n","category":"function"},{"location":"dtable/#Base.getindex","page":"Distributed Table","title":"Base.getindex","text":"getindex(gdt::GDTable, key) -> DTable\n\nRetrieves a DTable from gdt with rows belonging to the provided grouping key.\n\n\n\n\n\n","category":"function"},{"location":"processors/#Processors","page":"Processors","title":"Processors","text":"","category":"section"},{"location":"processors/","page":"Processors","title":"Processors","text":"Dagger contains a flexible mechanism to represent CPUs, GPUs, and other devices that the scheduler can place user work on. The individual devices that are capable of computing a user operation are called \"processors\", and are subtypes of Dagger.Processor. Processors are automatically detected by Dagger at scheduler initialization, and placed in a hierarchy reflecting the physical (network-, link-, or memory-based) boundaries between processors in the hierarchy. The scheduler uses the information in this hierarchy to efficiently schedule and partition user operations.","category":"page"},{"location":"processors/","page":"Processors","title":"Processors","text":"Dagger's Chunk objects can have a processor associated with them that defines where the contained data \"resides\". Each processor has a set of functions that define the mechanisms and rules by which the data can be transferred between similar or different kinds of processors, and will be called by Dagger's scheduler automatically when fetching function arguments (or the function itself) for computation on a given processor.","category":"page"},{"location":"processors/","page":"Processors","title":"Processors","text":"Setting the processor on a function argument is done by wrapping it in a Chunk with Dagger.tochunk:","category":"page"},{"location":"processors/","page":"Processors","title":"Processors","text":"a = 1\nb = 2\n# Let's say `b` \"resides\" on the second thread of the first worker:\nb_chunk = Dagger.tochunk(b, Dagger.ThreadProc(1, 2))::Dagger.Chunk\nc = Dagger.@spawn a + b_chunk\nfetch(c) == 3","category":"page"},{"location":"processors/","page":"Processors","title":"Processors","text":"It's also simple to set the processor of the function being passed; it will be automatically wrapped in a Chunk if necessary:","category":"page"},{"location":"processors/","page":"Processors","title":"Processors","text":"# `+` is treated as existing on the second thread of the first worker:\nDagger.@spawn processor=Dagger.ThreadProc(1, 2) a + b","category":"page"},{"location":"processors/","page":"Processors","title":"Processors","text":"You can also tell Dagger about the processor type for the returned value of a task by making it a Chunk:","category":"page"},{"location":"processors/","page":"Processors","title":"Processors","text":"Dagger.spawn(a) do a\n    c = a + 1\n    return Dagger.tochunk(c, Dagger.ThreadProc(1, 2))\nend","category":"page"},{"location":"processors/","page":"Processors","title":"Processors","text":"Note that unless you know that your function, arguments, or return value are associated with a specific processor, you don't need to assign one to them. Dagger will treat them as being simple values with no processor association, and will serialize them to wherever they're used.","category":"page"},{"location":"processors/#Hardware-capabilities,-topology,-and-data-locality","page":"Processors","title":"Hardware capabilities, topology, and data locality","text":"","category":"section"},{"location":"processors/","page":"Processors","title":"Processors","text":"The processor hierarchy is modeled as a multi-root tree, where each root is an OSProc, which represents a Julia OS process, and the \"children\" of the root or some other branch in the tree represent the processors which reside on the same logical server as the \"parent\" branch. All roots are connected to each other directly, in the common case. The processor hierarchy's topology is automatically detected and elaborated by callbacks in Dagger, which users may manipulate to add detection of extra processors.","category":"page"},{"location":"processors/","page":"Processors","title":"Processors","text":"A move between a given pair of processors is implemented as a Julia function dispatching on the types of each processor, as well as the type of the data being moved. Users are permitted to define custom move functions to improve data movement efficiency, perform automatic value conversions, or even make use of special IPC facilities. Custom processors may also be defined by the user to represent a processor type which is not automatically detected by Dagger, such as novel GPUs, special OS process abstractions, FPGAs, etc.","category":"page"},{"location":"processors/","page":"Processors","title":"Processors","text":"Movement of data between any two processors A and B (from A to B), if not defined by the user, is decomposed into 3 moves: processor A to OSProc parent of A, OSProc parent of A to OSProc parent of B, and OSProc parent of B to processor B. This mechanism uses Julia's Serialization library to serialize and deserialize data, so data must be serializable for this mechanism to work properly.","category":"page"},{"location":"processors/#Future:-Hierarchy-Generic-Path-Move","page":"Processors","title":"Future: Hierarchy Generic Path Move","text":"","category":"section"},{"location":"processors/","page":"Processors","title":"Processors","text":"NOTE: This used to be the default move behavior, but was removed because it wasn't considered helpful, and there were not any processor implementations that made use of it.","category":"page"},{"location":"processors/","page":"Processors","title":"Processors","text":"Movement of data between any two processors is decomposable into a sequence of \"moves\" between a child and its parent, termed a \"generic path move\". Movement of data may also take \"shortcuts\" between nodes in the tree which are not directly connected if enabled by libraries or the user, which may make use of IPC mechanisms to transfer data more directly and efficiently (such as Infiniband, GPU RDMA, NVLINK, etc.). All data is considered local to some processor, and may only be operated on by another processor by first doing an explicit move operation to that processor.","category":"page"},{"location":"processors/#Processor-Selection","page":"Processors","title":"Processor Selection","text":"","category":"section"},{"location":"processors/","page":"Processors","title":"Processors","text":"By default, Dagger uses the CPU to process work, typically single-threaded per cluster node. However, Dagger allows access to a wider range of hardware and software acceleration techniques, such as multithreading and GPUs. These more advanced (but performant) accelerators are disabled by default, but can easily be enabled by using Scheduler/Thunk options in the proclist field. If nothing, all default processors will be used. If a vector of types, only the processor types contained in options.proclist will be used to compute all or a given thunk. If a function, it will be called for each processor (with the processor as the argument) until it returns true.","category":"page"},{"location":"processors/","page":"Processors","title":"Processors","text":"opts = Dagger.Sch.ThunkOptions(;proclist=nothing) # default behavior\n# OR\nopts = Dagger.Sch.ThunkOptions(;proclist=[DaggerGPU.CuArrayProc]) # only execute on CuArrayProc\n# OR\nopts = Dagger.Sch.ThunkOptions(;proclist=(proc)->(proc isa Dagger.ThreadProc && proc.tid == 3)) # only run on ThreadProc with thread ID 3\n\nt = Dagger.@par options=opts sum(X) # do sum(X) on the specified processor","category":"page"},{"location":"processors/#Resource-Control","page":"Processors","title":"Resource Control","text":"","category":"section"},{"location":"processors/","page":"Processors","title":"Processors","text":"Dagger assumes that a thunk executing on a processor, fully utilizes that processor at 100%. When this is not the case, you can tell Dagger as much with options.procutil:","category":"page"},{"location":"processors/","page":"Processors","title":"Processors","text":"procutil = Dict(\n    Dagger.ThreadProc => 4.0, # utilizes 4 CPU threads fully\n    DaggerGPU.CuArrayProc => 0.1 # utilizes 10% of a single CUDA GPU\n)","category":"page"},{"location":"processors/","page":"Processors","title":"Processors","text":"Dagger will use this information to execute only as many thunks on a given processor (or set of similar processors) as add up to less than or equal to 1.0 total utilization. If a thunk is scheduled onto a processor which the local worker deems as \"oversubscribed\", it will not execute the thunk until sufficient resources become available by thunks completing execution.","category":"page"},{"location":"processors/#GPU-Processors","page":"Processors","title":"GPU Processors","text":"","category":"section"},{"location":"processors/","page":"Processors","title":"Processors","text":"The DaggerGPU.jl package can be imported to enable GPU acceleration for NVIDIA and AMD GPUs, when available. The processors provided by that package are not enabled by default, but may be enabled via options.proclist as usual.","category":"page"},{"location":"processors/#Future:-Network-Devices-and-Topology","page":"Processors","title":"Future: Network Devices and Topology","text":"","category":"section"},{"location":"processors/","page":"Processors","title":"Processors","text":"In the future, users will be able to define network devices attached to a given processor, which provides a direct connection to a network device on another processor, and may be used to transfer data between said processors. Data movement rules will most likely be defined by a similar (or even identical) mechanism to the current processor move mechanism. The multi-root tree will be expanded to a graph to allow representing these network devices (as they may potentially span non-root nodes).","category":"page"},{"location":"processors/#Redundancy","page":"Processors","title":"Redundancy","text":"","category":"section"},{"location":"processors/#Fault-Tolerance","page":"Processors","title":"Fault Tolerance","text":"","category":"section"},{"location":"processors/","page":"Processors","title":"Processors","text":"Dagger has a single means for ensuring redundancy, which is currently called \"fault tolerance\". Said redundancy is only targeted at a specific failure mode, namely the unexpected exit or \"killing\" of a worker process in the cluster. This failure mode often presents itself when running on a Linux and generating large memory allocations, where the Out Of Memory (OOM) killer process can kill user processes to free their allocated memory for the Linux kernel to use. The fault tolerance system mitigates the damage caused by the OOM killer performing its duties on one or more worker processes by detecting the fault as a process exit exception (generated by Julia), and then moving any \"lost\" work to other worker processes for re-computation.","category":"page"},{"location":"processors/#Future:-Multi-master,-Network-Failure-Correction,-etc.","page":"Processors","title":"Future: Multi-master, Network Failure Correction, etc.","text":"","category":"section"},{"location":"processors/","page":"Processors","title":"Processors","text":"This single redundancy mechanism helps alleviate a common issue among HPC and scientific users, however it does little to help when, for example, the master node exits, or a network link goes down. Such failure modes require a more complicated detection and recovery process, including multiple master processes, a distributed and replicated database such as etcd, and checkpointing of the scheduler to ensure an efficient recovery. Such a system does not yet exist, but contributions for such a change are desired.","category":"page"},{"location":"processors/#Dynamic-worker-pools","page":"Processors","title":"Dynamic worker pools","text":"","category":"section"},{"location":"processors/","page":"Processors","title":"Processors","text":"Dagger's default scheduler supports modifying the worker pool while the scheduler is running. This is done by modifying the Processors of the Context supplied to the scheduler at initialization using addprocs!(ctx, ps) and rmprocs(ctx, ps) where ps can be Processors or just process ids.","category":"page"},{"location":"processors/","page":"Processors","title":"Processors","text":"An example of when this is useful is in HPC environments where individual jobs to start up workers are queued so that not all workers are guaranteed to be available at the same time.","category":"page"},{"location":"processors/","page":"Processors","title":"Processors","text":"New workers will typically be assigned new tasks as soon as the scheduler sees them. Removed workers will finish all their assigned tasks but will not be assigned any new tasks. Note that this makes it difficult to determine when a worker is no longer in use by Dagger. Contributions to alleviate this uncertainty are welcome!","category":"page"},{"location":"processors/","page":"Processors","title":"Processors","text":"Example:","category":"page"},{"location":"processors/","page":"Processors","title":"Processors","text":"using Distributed\n\nps1 = addprocs(2, exeflags=\"--project\")\n@everywhere using Distributed, Dagger\n\n# Dummy task to wait for 0.5 seconds and then return the id of the worker\nts = delayed(vcat)((delayed(i -> (sleep(0.5); myid()))(i) for i in 1:20)...)\n\nctx = Context()\n# Scheduler is blocking, so we need a new task to add workers while it runs\njob = @async collect(ctx, ts)\n\n# Lets fire up some new workers\nps2 = addprocs(2, exeflags=\"--project\")\n@everywhere ps2 using Distributed, Dagger\n# New workers are not available until we do this\naddprocs!(ctx, ps2)\n\n# Lets hope the job didn't complete before workers were added :)\n@show fetch(job) |> unique\n\n# and cleanup after ourselves...\nworkers() |> rmprocs","category":"page"},{"location":"#A-framework-for-out-of-core-and-parallel-execution","page":"Home","title":"A framework for out-of-core and parallel execution","text":"","category":"section"},{"location":"#Usage","page":"Home","title":"Usage","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The main function for using Dagger is delayed","category":"page"},{"location":"","page":"Home","title":"Home","text":"delayed(f; options...)","category":"page"},{"location":"","page":"Home","title":"Home","text":"It returns a function which when called creates a Thunk object representing a call to function f with the given arguments. If it is called with other thunks as input, then they form a graph with input nodes directed at the output. The function f gets the result of the input Thunks. Thunks don't pass keyword argument to the function f. Options kwargs... to delayed are passed to the scheduler to control its behavior:","category":"page"},{"location":"","page":"Home","title":"Home","text":"get_result::Bool – return the actual result to the scheduler instead of Chunk objects. Used when f explicitly constructs a Chunk or when return value is small (e.g. in case of reduce)\nmeta::Bool – pass the input “Chunk” objects themselves to f and not the value contained in them - this is always run on the master process\npersist::Bool – the result of this Thunk should not be released after it becomes unused in the DAG\ncache::Bool – cache the result of this Thunk such that if the thunk is evaluated again, one can just reuse the cached value. If it’s been removed from cache, recompute the value.","category":"page"},{"location":"#DAG-creation-interface","page":"Home","title":"DAG creation interface","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Here is a very simple example DAG:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Dagger\n\nadd1(value) = value + 1\nadd2(value) = value + 2\ncombine(a...) = sum(a)\n\np = delayed(add1)(4)\nq = delayed(add2)(p)\nr = delayed(add1)(3)\ns = delayed(combine)(p, q, r)\n\n@assert collect(s) == 16","category":"page"},{"location":"","page":"Home","title":"Home","text":"The above computation can also be written in a more Julia-idiomatic syntax with @par:","category":"page"},{"location":"","page":"Home","title":"Home","text":"p = @par add1(4)\nq = @par add2(p)\nr = @par add1(3)\ns = @par combine(p, q, r)\n\n@assert collect(s) == 16","category":"page"},{"location":"","page":"Home","title":"Home","text":"or similarly:","category":"page"},{"location":"","page":"Home","title":"Home","text":"s = @par begin\n    p = add1(4)\n    q = add2(p)\n    r = add1(3)\n    combine(p, q, r)\nend\n\n@assert collect(s) == 16","category":"page"},{"location":"","page":"Home","title":"Home","text":"The connections between nodes p, q, r and s is represented by this dependency graph:","category":"page"},{"location":"","page":"Home","title":"Home","text":"(Image: graph)","category":"page"},{"location":"","page":"Home","title":"Home","text":"The final result is the obvious consequence of the operation","category":"page"},{"location":"","page":"Home","title":"Home","text":"add1(4) + add2(add1(4)) + add1(3)","category":"page"},{"location":"","page":"Home","title":"Home","text":"(4 + 1) + ((4 + 1) + 2) + (3 + 1) = 16","category":"page"},{"location":"","page":"Home","title":"Home","text":"To compute and fetch the result of a thunk (say s), you can call collect(s). collect will fetch the result of the computation to the master process. Alternatively, if you want to compute but not fetch the result you can call compute on the thunk. This will return a Chunk object which references the result. If you pass in a Chunk objects as an input to a delayed function, then the function will get executed with the value of the Chunk – this evaluation will likely happen where the input chunks are, to reduce communication.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The key point is that, for each argument to a node, if the argument is a Thunk, it'll be executed before this node and its result will be passed into the function f provided. If the argument is not a Thunk (just some regular Julia object), it'll be passed as-is to the function f.","category":"page"},{"location":"#Polytree","page":"Home","title":"Polytree","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Polytrees are easily supported by Dagger. To make this work, pass all the head nodes Thunks into a call to delayed as arguments, which will act as the top node for the graph.","category":"page"},{"location":"","page":"Home","title":"Home","text":"group(x...) = [x...]\ntop_node = delayed(group)(head_nodes...)\ncompute(top_node)","category":"page"},{"location":"#Eager-Execution","page":"Home","title":"Eager Execution","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Similar to @par, Dagger has an @spawn macro (and matching Dagger.spawn) which works similarly to @async and Threads.@spawn: when called, it wraps the function call specified by the user in an EagerThunk object, and immediately places it onto a running scheduler, to be executed once its dependencies are fulfilled. This contrasts with @par in that @par does not begin executing its thunks until collect or compute are called on a given thunk or one of its downstream dependencies. Additionally, one fetches the result of any @spawn call with fetch. As a concrete example:","category":"page"},{"location":"","page":"Home","title":"Home","text":"x = rand(400,400)\ny = rand(400,400)\nzt = Dagger.@spawn x * y\nz = fetch(zt)","category":"page"},{"location":"","page":"Home","title":"Home","text":"One can also wait on the result of @spawn and check completion status with isready:","category":"page"},{"location":"","page":"Home","title":"Home","text":"x = Dagger.@spawn sleep(10)\n@assert !isready(x)\nwait(x)\n@assert isready(x)\n@info \"Done!\"","category":"page"},{"location":"","page":"Home","title":"Home","text":"One can also safely call @spawn from another worker (not id 1), and it will be sent to worker 1 to schedule:","category":"page"},{"location":"","page":"Home","title":"Home","text":"x = fetch(Distributed.@spawnat 2 Dagger.@spawn 1+2) # actually scheduled on worker 1\nx::EagerThunk\n@assert fetch(x) == 3","category":"page"},{"location":"","page":"Home","title":"Home","text":"This is useful for nested execution, where an @spawn'd thunk calls @spawn.","category":"page"},{"location":"","page":"Home","title":"Home","text":"If a thunk errors while running under the eager scheduler, it will be marked as having failed, all dependent (downstream) thunks will be marked as failed, and any future thunks that use a failed thunk as input will fail. Failure can be determined with fetch, which will re-throw the error that the originally-failing thunk threw. wait and isready will not check whether a thunk or its upstream failed; they only check if the thunk has completed, error or not.","category":"page"},{"location":"","page":"Home","title":"Home","text":"This failure behavior is not the default for lazy scheduling, but can be enabled by setting the scheduler/thunk option (see below) allow_error to true.  However, this option isn't terribly useful for non-dynamic usecases, since any thunk failure will propagate down to the output thunk regardless of where it occurs.","category":"page"},{"location":"#Scheduler-and-Thunk-options","page":"Home","title":"Scheduler and Thunk options","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"While Dagger generally \"just works\", sometimes one needs to exert some more fine-grained control over how the scheduler allocates work. There are two parallel mechanisms to achieve this: Scheduler options (from Dagger.Sch.SchedulerOptions) and Thunk options (from Dagger.Sch.ThunkOptions). These two options structs generally contain the same options, with the difference being that Scheduler options operate globally across an entire DAG, and Thunk options operate on a thunk-by-thunk basis. Scheduler options can be constructed and passed to collect() or compute() as the keyword argument options, and Thunk options can be passed to Dagger's delayed function similarly: delayed(myfunc)(arg1, arg2, ...; options=opts). Check the docstring for the two options structs to see what options are available.","category":"page"},{"location":"#Rough-high-level-description-of-scheduling","page":"Home","title":"Rough high level description of scheduling","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"First picks the leaf Thunks and distributes them to available workers. Each worker is given at most 1 task at a time. If input to the node is a Chunk, then workers which already have the chunk are preferred.\nWhen a worker finishes a thunk it will return a Chunk object to the scheduler.\nOnce the worker has returned a Chunk, the scheduler picks the next task for the worker – this is usually the task the worker immediately made available (if possible). In the small example above, if worker 2 finished p it will be given q since it will already have the result of p which is input to q.\nThe scheduler also issues \"release\" Commands to chunks that are no longer required by nodes in the DAG: for example, when s is computed all of p, q, r are released to free up memory. This can be prevented by passing persist or cache options to delayed.","category":"page"},{"location":"#Modeling-of-Dagger-programs","page":"Home","title":"Modeling of Dagger programs","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The key API for parallel and heterogeneous execution is Dagger.delayed. The call signature of Dagger.delayed is the following:","category":"page"},{"location":"","page":"Home","title":"Home","text":"thunk = Dagger.delayed(func)(args...)","category":"page"},{"location":"","page":"Home","title":"Home","text":"This invocation serves to construct a single node in a computational graph. func is a Julia function, which normally takes some number of arguments, of length N and of types Targs. The set of arguments args... is specified with ellipses to indicate that many arguments may be passed between the parentheses. When correctly invoked, args... is of length N and of types Targs (or suitable subtypes of Targs, for each respective argument in args...).  thunk is an instance of a Dagger Thunk, which is the value used internally by Dagger to represent a node in the graph.","category":"page"},{"location":"","page":"Home","title":"Home","text":"A Thunk may be \"computed\":","category":"page"},{"location":"","page":"Home","title":"Home","text":"chunk = Dagger.compute(thunk)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Computing a Thunk performs roughly the same logic as the following Julia function invocation:","category":"page"},{"location":"","page":"Home","title":"Home","text":"result = func(args...)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Such an invocation invokes func on args..., returning result. Computing the above thunk would produce a value with the same type as result, with the caveat that the result will be wrapped by a Dagger.Chunk (chunk in the above example). A Chunk is a reference to a value stored on a compute process within the Distributed cluster that Dagger is operating within. A Chunk may be \"collected\", which will return the wrapped value to the collecting process, which in the above example will be result:","category":"page"},{"location":"","page":"Home","title":"Home","text":"result = collect(chunk)","category":"page"},{"location":"","page":"Home","title":"Home","text":"In order to create a graph with more than a single node, arguments to delayed may themselves be Thunks or Chunks. For example, the sum of the elements of vector [1,2,3,4] may be represented in Dagger as follows:","category":"page"},{"location":"","page":"Home","title":"Home","text":"thunk1 = Dagger.delayed(+)(1, 2)\nthunk2 = Dagger.delayed(+)(3, 4)\nthunk3 = Dagger.delayed(+)(thunk1, thunk2)","category":"page"},{"location":"","page":"Home","title":"Home","text":"A graph has now been constructed, where thunk1 and thunk2 are dependencies (\"inputs\") to thunk3. Computing thunk3 and then collecting its resulting Chunk would result in the answer that is expected from the operation:","category":"page"},{"location":"","page":"Home","title":"Home","text":"chunk = compute(thunk3)\nresult = collect(chunk)","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> result == 10\ntrue","category":"page"},{"location":"","page":"Home","title":"Home","text":"result now has the Int64 value 10, which is the result of summing the elements of the vector [1,2,3,4]. For convenience, computation may be performed together with collection, like so:","category":"page"},{"location":"","page":"Home","title":"Home","text":"result = collect(thunk3)","category":"page"},{"location":"","page":"Home","title":"Home","text":"The above summation example is equivalent to the following invocation in plain Julia:","category":"page"},{"location":"","page":"Home","title":"Home","text":"x1 = 1 + 2\nx2 = 3 + 4\nresult = x1 + x2\nresult == 10","category":"page"},{"location":"","page":"Home","title":"Home","text":"However, there are key differences when using Dagger to perform this operation as compared to performing this operation without Dagger. In Dagger, the graph is constructed separately from computing the graph (\"lazily\"), whereas without Dagger the graph is executed immediately (\"eagerly\"). Dagger makes use of this lazy construction approach to allow modifying the actual execution of the overall operation in useful ways.","category":"page"},{"location":"","page":"Home","title":"Home","text":"By default, computing a Dagger graph creates an instance of a scheduler, which will be provided the graph to execute. The scheduler executes the individual nodes of the graph on their arguments in the order specified by the graph (ensuring dependencies to a node are satisfied before executing said node) on compute processes in the cluster; the scheduler process itself typically does not execute the nodes directly. Additionally, if a given set of nodes do not depend on each other (the value generated by a node is not an input to another node in the set), then those nodes may be executed in parallel, and the scheduler attempts to schedule such nodes in parallel when possible.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The scheduler also orchestrates data movement between compute processes, such that inputs to a given node are available on the compute process that is scheduled to execute said node. The scheduler attempts to minimize data movement between compute processes; it does so by trying to schedule nodes which depend on a given input on the same compute process that computed and retains that input.","category":"page"}]
}
